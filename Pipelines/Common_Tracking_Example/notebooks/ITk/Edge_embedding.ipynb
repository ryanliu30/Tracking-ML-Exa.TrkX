{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " weight\n"
     ]
    }
   ],
   "source": [
    "run_name = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# External imports\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import frnn\n",
    "import wandb\n",
    "import math\n",
    "sys.path.append('../..')\n",
    "\n",
    "from LightningModules.EdgeEmbedding.Models.vanilla_edge_embedding import VanillaEdgeEmbedding\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        elif name.startswith(\"layers.0\"):  # The first layer does not have ReLU applied on its input\n",
    "            param.data.normal_(0, 1 / math.sqrt(param.shape[1]))\n",
    "        else:\n",
    "            param.data.normal_(0, math.sqrt(2) / math.sqrt(param.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edge_embedding_sweep.yaml\") as f:\n",
    "        sweep_hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "with open(\"edge_embedding_default.yaml\") as f:\n",
    "        default_hparams = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    \"name\": run_name,\n",
    "    \"project\": \"ITk_barrell_edge_embedding\",\n",
    "    \"metric\": {\"name\": \"pur\", \"goal\": \"maximize\"},\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": sweep_hparams\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    wandb.init()\n",
    "    model = VanillaEdgeEmbedding({**default_hparams, **wandb.config})\n",
    "    kaiming_init(model)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='pur',\n",
    "        mode=\"max\",\n",
    "        save_top_k=2,\n",
    "        save_last=True)\n",
    "\n",
    "    logger = WandbLogger()\n",
    "    trainer = Trainer(gpus=1, max_epochs=default_hparams[\"max_epochs\"], log_every_n_steps = 10, logger=logger, callbacks=[checkpoint_callback], default_root_dir=\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_doublet_embedding/\")\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: r7td01s0\n",
      "Sweep URL: https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j0r2tkdm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mexatrkx\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/j0r2tkdm\" target=\"_blank\">spring-sweep-1</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 23.6 K\n",
      "2 | input_layer2  | Linear           | 23.6 K\n",
      "3 | layers1       | ModuleList       | 4.2 M \n",
      "4 | layers2       | ModuleList       | 4.2 M \n",
      "5 | output_layer1 | Linear           | 16.4 K\n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "8.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.5 M     Total params\n",
      "33.907    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:46<00:46, 46.43s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 200/202 [11:32<00:06,  3.46s/it, loss=0.0811, v_num=tkdm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 202/202 [12:12<00:00,  3.63s/it, loss=0.0811, v_num=tkdm]\n",
      "Epoch 0: 100%|██████████| 202/202 [12:44<00:00,  3.78s/it, loss=0.0811, v_num=tkdm]\n",
      "Epoch 1:  99%|█████████▉| 200/202 [11:38<00:06,  3.49s/it, loss=0.0804, v_num=tkdm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 202/202 [12:19<00:00,  3.66s/it, loss=0.0804, v_num=tkdm]\n",
      "Epoch 1: 100%|██████████| 202/202 [12:50<00:00,  3.81s/it, loss=0.0804, v_num=tkdm]\n",
      "Epoch 2:  99%|█████████▉| 200/202 [11:40<00:07,  3.50s/it, loss=0.08, v_num=tkdm]  \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 202/202 [12:21<00:00,  3.67s/it, loss=0.08, v_num=tkdm]\n",
      "Epoch 2: 100%|██████████| 202/202 [12:52<00:00,  3.83s/it, loss=0.08, v_num=tkdm]\n",
      "Epoch 3:  99%|█████████▉| 200/202 [11:50<00:07,  3.55s/it, loss=0.0797, v_num=tkdm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 202/202 [12:31<00:00,  3.72s/it, loss=0.0797, v_num=tkdm]\n",
      "Epoch 3: 100%|██████████| 202/202 [13:03<00:00,  3.88s/it, loss=0.0797, v_num=tkdm]\n",
      "Epoch 4:  99%|█████████▉| 200/202 [12:02<00:07,  3.61s/it, loss=0.0797, v_num=tkdm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 202/202 [12:43<00:00,  3.78s/it, loss=0.0797, v_num=tkdm]\n",
      "Epoch 4: 100%|██████████| 202/202 [13:14<00:00,  3.93s/it, loss=0.0797, v_num=tkdm]\n",
      "Epoch 5:  99%|█████████▉| 200/202 [11:48<00:07,  3.54s/it, loss=0.0793, v_num=tkdm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 202/202 [12:33<00:00,  3.73s/it, loss=0.0793, v_num=tkdm]\n",
      "Epoch 5: 100%|██████████| 202/202 [13:08<00:00,  3.90s/it, loss=0.0793, v_num=tkdm]\n",
      "Epoch 6:  99%|█████████▉| 200/202 [11:23<00:06,  3.42s/it, loss=0.0792, v_num=tkdm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 202/202 [12:08<00:00,  3.61s/it, loss=0.0792, v_num=tkdm]\n",
      "Epoch 6: 100%|██████████| 202/202 [12:44<00:00,  3.78s/it, loss=0.0792, v_num=tkdm]\n",
      "Epoch 7:  99%|█████████▉| 200/202 [11:53<00:07,  3.57s/it, loss=0.0793, v_num=tkdm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 202/202 [12:38<00:00,  3.76s/it, loss=0.0793, v_num=tkdm]\n",
      "Epoch 7: 100%|██████████| 202/202 [13:14<00:00,  3.93s/it, loss=0.0793, v_num=tkdm]\n",
      "Epoch 8:  99%|█████████▉| 200/202 [11:59<00:07,  3.60s/it, loss=0.0792, v_num=tkdm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 202/202 [12:44<00:00,  3.79s/it, loss=0.0792, v_num=tkdm]\n",
      "Epoch 8: 100%|██████████| 202/202 [13:20<00:00,  3.96s/it, loss=0.0792, v_num=tkdm]\n",
      "Epoch 9:  99%|█████████▉| 200/202 [12:15<00:07,  3.68s/it, loss=0.0791, v_num=tkdm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 202/202 [13:00<00:00,  3.87s/it, loss=0.0791, v_num=tkdm]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:36<00:00,  4.04s/it, loss=0.0791, v_num=tkdm]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:37<00:00,  4.05s/it, loss=0.0791, v_num=tkdm]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 239743... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_pur</td><td>▁▁▄▅▅▇▇███</td></tr><tr><td>dist@0.8</td><td>▄▅▁▅█▂▄▂▁▂</td></tr><tr><td>eff</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▁▄▄▅▆▆▇██</td></tr><tr><td>train_loss</td><td>█▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>1e-05</td></tr><tr><td>cut_pur</td><td>0.00174</td></tr><tr><td>dist@0.8</td><td>0.87419</td></tr><tr><td>eff</td><td>0.7383</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00146</td></tr><tr><td>train_loss</td><td>0.07895</td></tr><tr><td>trainer/global_step</td><td>1999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">spring-sweep-1</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/j0r2tkdm\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/j0r2tkdm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220224_163817-j0r2tkdm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 99tuvhg4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/99tuvhg4\" target=\"_blank\">dulcet-sweep-2</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 23.6 K\n",
      "2 | input_layer2  | Linear           | 23.6 K\n",
      "3 | layers1       | ModuleList       | 4.2 M \n",
      "4 | layers2       | ModuleList       | 4.2 M \n",
      "5 | output_layer1 | Linear           | 16.4 K\n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "8.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.5 M     Total params\n",
      "33.907    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 200/202 [12:03<00:07,  3.62s/it, loss=0.135, v_num=vhg4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 202/202 [12:48<00:00,  3.81s/it, loss=0.135, v_num=vhg4]\n",
      "Epoch 0: 100%|██████████| 202/202 [13:24<00:00,  3.98s/it, loss=0.135, v_num=vhg4]\n",
      "Epoch 1:  99%|█████████▉| 200/202 [11:56<00:07,  3.58s/it, loss=0.133, v_num=vhg4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 202/202 [12:42<00:00,  3.78s/it, loss=0.133, v_num=vhg4]\n",
      "Epoch 1: 100%|██████████| 202/202 [13:18<00:00,  3.95s/it, loss=0.133, v_num=vhg4]\n",
      "Epoch 2:  99%|█████████▉| 200/202 [11:49<00:07,  3.55s/it, loss=0.132, v_num=vhg4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 202/202 [12:31<00:00,  3.72s/it, loss=0.132, v_num=vhg4]\n",
      "Epoch 2: 100%|██████████| 202/202 [13:02<00:00,  3.88s/it, loss=0.132, v_num=vhg4]\n",
      "Epoch 3:  99%|█████████▉| 200/202 [12:04<00:07,  3.62s/it, loss=0.131, v_num=vhg4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 202/202 [12:51<00:00,  3.82s/it, loss=0.131, v_num=vhg4]\n",
      "Epoch 3: 100%|██████████| 202/202 [13:27<00:00,  4.00s/it, loss=0.131, v_num=vhg4]\n",
      "Epoch 4:  99%|█████████▉| 200/202 [12:26<00:07,  3.73s/it, loss=0.131, v_num=vhg4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 202/202 [13:07<00:00,  3.90s/it, loss=0.131, v_num=vhg4]\n",
      "Epoch 4: 100%|██████████| 202/202 [13:39<00:00,  4.06s/it, loss=0.131, v_num=vhg4]\n",
      "Epoch 5:  99%|█████████▉| 200/202 [12:09<00:07,  3.65s/it, loss=0.13, v_num=vhg4] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 202/202 [12:50<00:00,  3.82s/it, loss=0.13, v_num=vhg4]\n",
      "Epoch 5: 100%|██████████| 202/202 [13:22<00:00,  3.97s/it, loss=0.13, v_num=vhg4]\n",
      "Epoch 6:  99%|█████████▉| 200/202 [12:24<00:07,  3.72s/it, loss=0.13, v_num=vhg4] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 202/202 [13:05<00:00,  3.89s/it, loss=0.13, v_num=vhg4]\n",
      "Epoch 6: 100%|██████████| 202/202 [13:37<00:00,  4.05s/it, loss=0.13, v_num=vhg4]\n",
      "Epoch 7:  99%|█████████▉| 200/202 [11:57<00:07,  3.59s/it, loss=0.13, v_num=vhg4] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 202/202 [12:40<00:00,  3.77s/it, loss=0.13, v_num=vhg4]\n",
      "Epoch 7: 100%|██████████| 202/202 [13:11<00:00,  3.92s/it, loss=0.13, v_num=vhg4]\n",
      "Epoch 8:  99%|█████████▉| 200/202 [12:02<00:07,  3.61s/it, loss=0.129, v_num=vhg4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 202/202 [12:48<00:00,  3.80s/it, loss=0.129, v_num=vhg4]\n",
      "Epoch 8: 100%|██████████| 202/202 [13:24<00:00,  3.98s/it, loss=0.129, v_num=vhg4]\n",
      "Epoch 9:  99%|█████████▉| 200/202 [12:00<00:07,  3.60s/it, loss=0.129, v_num=vhg4]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 202/202 [12:46<00:00,  3.80s/it, loss=0.129, v_num=vhg4]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:22<00:00,  3.97s/it, loss=0.129, v_num=vhg4]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:22<00:00,  3.98s/it, loss=0.129, v_num=vhg4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 64945... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_pur</td><td>▁▂▃▅▆▇▇▇██</td></tr><tr><td>dist@0.8</td><td>▆▄█▁▃▄▅▇▃▅</td></tr><tr><td>eff</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▂▃▅▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>1e-05</td></tr><tr><td>cut_pur</td><td>0.00202</td></tr><tr><td>dist@0.8</td><td>0.80785</td></tr><tr><td>eff</td><td>0.7383</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00163</td></tr><tr><td>train_loss</td><td>0.12779</td></tr><tr><td>trainer/global_step</td><td>1999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">dulcet-sweep-2</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/99tuvhg4\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/99tuvhg4</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220224_185057-99tuvhg4/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s7o9o0vv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.3\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/s7o9o0vv\" target=\"_blank\">noble-sweep-3</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 23.6 K\n",
      "2 | input_layer2  | Linear           | 23.6 K\n",
      "3 | layers1       | ModuleList       | 4.2 M \n",
      "4 | layers2       | ModuleList       | 4.2 M \n",
      "5 | output_layer1 | Linear           | 16.4 K\n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "8.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.5 M     Total params\n",
      "33.907    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 200/202 [12:07<00:07,  3.64s/it, loss=0.169, v_num=o0vv]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 202/202 [12:49<00:00,  3.81s/it, loss=0.169, v_num=o0vv]\n",
      "Epoch 0: 100%|██████████| 202/202 [13:20<00:00,  3.96s/it, loss=0.169, v_num=o0vv]\n",
      "Epoch 1:  99%|█████████▉| 200/202 [12:13<00:07,  3.67s/it, loss=0.166, v_num=o0vv]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 202/202 [12:59<00:00,  3.86s/it, loss=0.166, v_num=o0vv]\n",
      "Epoch 1: 100%|██████████| 202/202 [13:35<00:00,  4.04s/it, loss=0.166, v_num=o0vv]\n",
      "Epoch 2:  99%|█████████▉| 200/202 [12:26<00:07,  3.73s/it, loss=0.166, v_num=o0vv]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 202/202 [13:08<00:00,  3.90s/it, loss=0.166, v_num=o0vv]\n",
      "Epoch 2: 100%|██████████| 202/202 [13:40<00:00,  4.06s/it, loss=0.166, v_num=o0vv]\n",
      "Epoch 3:  99%|█████████▉| 200/202 [12:18<00:07,  3.69s/it, loss=0.164, v_num=o0vv]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 202/202 [13:04<00:00,  3.88s/it, loss=0.164, v_num=o0vv]\n",
      "Epoch 3: 100%|██████████| 202/202 [13:41<00:00,  4.07s/it, loss=0.164, v_num=o0vv]\n",
      "Epoch 4:  99%|█████████▉| 200/202 [12:49<00:07,  3.85s/it, loss=0.163, v_num=o0vv]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 202/202 [13:35<00:00,  4.04s/it, loss=0.163, v_num=o0vv]\n",
      "Epoch 4: 100%|██████████| 202/202 [14:11<00:00,  4.22s/it, loss=0.163, v_num=o0vv]\n",
      "Epoch 5:  99%|█████████▉| 200/202 [12:12<00:07,  3.66s/it, loss=0.162, v_num=o0vv]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 202/202 [12:58<00:00,  3.85s/it, loss=0.162, v_num=o0vv]\n",
      "Epoch 5: 100%|██████████| 202/202 [13:33<00:00,  4.03s/it, loss=0.162, v_num=o0vv]\n",
      "Epoch 6:  99%|█████████▉| 200/202 [12:02<00:07,  3.61s/it, loss=0.161, v_num=o0vv]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 202/202 [12:47<00:00,  3.80s/it, loss=0.161, v_num=o0vv]\n",
      "Epoch 6: 100%|██████████| 202/202 [13:22<00:00,  3.97s/it, loss=0.161, v_num=o0vv]\n",
      "Epoch 7:  99%|█████████▉| 200/202 [12:12<00:07,  3.66s/it, loss=0.16, v_num=o0vv] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 202/202 [12:53<00:00,  3.83s/it, loss=0.16, v_num=o0vv]\n",
      "Epoch 7: 100%|██████████| 202/202 [13:25<00:00,  3.99s/it, loss=0.16, v_num=o0vv]\n",
      "Epoch 8:  99%|█████████▉| 200/202 [12:48<00:07,  3.84s/it, loss=0.16, v_num=o0vv] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 202/202 [13:29<00:00,  4.01s/it, loss=0.16, v_num=o0vv]\n",
      "Epoch 8: 100%|██████████| 202/202 [14:01<00:00,  4.16s/it, loss=0.16, v_num=o0vv]\n",
      "Epoch 9:  99%|█████████▉| 200/202 [12:12<00:07,  3.66s/it, loss=0.159, v_num=o0vv]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 202/202 [12:53<00:00,  3.83s/it, loss=0.159, v_num=o0vv]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:23<00:00,  3.98s/it, loss=0.159, v_num=o0vv]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:24<00:00,  3.98s/it, loss=0.159, v_num=o0vv]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 152400... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_pur</td><td>▁▃▄▆▆▇▇███</td></tr><tr><td>dist@0.8</td><td>▅▅▁▄▇▄█▄▇▇</td></tr><tr><td>eff</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▃▅▆▆▇█▇██</td></tr><tr><td>train_loss</td><td>█▃▃▃▃▂▂▂▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁▂▂▁▁▁▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>1e-05</td></tr><tr><td>cut_pur</td><td>0.00214</td></tr><tr><td>dist@0.8</td><td>0.75798</td></tr><tr><td>eff</td><td>0.7383</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00171</td></tr><tr><td>train_loss</td><td>0.16086</td></tr><tr><td>trainer/global_step</td><td>1999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">noble-sweep-3</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/s7o9o0vv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/s7o9o0vv</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220224_210622-s7o9o0vv/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: j43eamm2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.4\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/j43eamm2\" target=\"_blank\">happy-sweep-4</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 23.6 K\n",
      "2 | input_layer2  | Linear           | 23.6 K\n",
      "3 | layers1       | ModuleList       | 4.2 M \n",
      "4 | layers2       | ModuleList       | 4.2 M \n",
      "5 | output_layer1 | Linear           | 16.4 K\n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "8.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.5 M     Total params\n",
      "33.907    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 200/202 [12:03<00:07,  3.62s/it, loss=0.194, v_num=amm2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 202/202 [12:49<00:00,  3.81s/it, loss=0.194, v_num=amm2]\n",
      "Epoch 0: 100%|██████████| 202/202 [13:25<00:00,  3.99s/it, loss=0.194, v_num=amm2]\n",
      "Epoch 1:  99%|█████████▉| 200/202 [12:13<00:07,  3.67s/it, loss=0.189, v_num=amm2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 202/202 [12:59<00:00,  3.86s/it, loss=0.189, v_num=amm2]\n",
      "Epoch 1: 100%|██████████| 202/202 [13:34<00:00,  4.03s/it, loss=0.189, v_num=amm2]\n",
      "Epoch 2:  99%|█████████▉| 200/202 [12:13<00:07,  3.67s/it, loss=0.188, v_num=amm2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 202/202 [12:59<00:00,  3.86s/it, loss=0.188, v_num=amm2]\n",
      "Epoch 2: 100%|██████████| 202/202 [13:34<00:00,  4.03s/it, loss=0.188, v_num=amm2]\n",
      "Epoch 3:  99%|█████████▉| 200/202 [12:19<00:07,  3.70s/it, loss=0.187, v_num=amm2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 202/202 [13:05<00:00,  3.89s/it, loss=0.187, v_num=amm2]\n",
      "Epoch 3: 100%|██████████| 202/202 [13:40<00:00,  4.06s/it, loss=0.187, v_num=amm2]\n",
      "Epoch 4:  99%|█████████▉| 200/202 [12:12<00:07,  3.66s/it, loss=0.185, v_num=amm2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 202/202 [12:58<00:00,  3.85s/it, loss=0.185, v_num=amm2]\n",
      "Epoch 4: 100%|██████████| 202/202 [13:35<00:00,  4.04s/it, loss=0.185, v_num=amm2]\n",
      "Epoch 5:  99%|█████████▉| 200/202 [12:02<00:07,  3.61s/it, loss=0.183, v_num=amm2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 202/202 [12:48<00:00,  3.80s/it, loss=0.183, v_num=amm2]\n",
      "Epoch 5: 100%|██████████| 202/202 [13:23<00:00,  3.98s/it, loss=0.183, v_num=amm2]\n",
      "Epoch 6:  99%|█████████▉| 200/202 [12:10<00:07,  3.65s/it, loss=0.183, v_num=amm2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 202/202 [12:55<00:00,  3.84s/it, loss=0.183, v_num=amm2]\n",
      "Epoch 6: 100%|██████████| 202/202 [13:31<00:00,  4.01s/it, loss=0.183, v_num=amm2]\n",
      "Epoch 7:  77%|███████▋  | 156/202 [09:31<02:48,  3.66s/it, loss=0.182, v_num=amm2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:  99%|█████████▉| 200/202 [12:08<00:07,  3.64s/it, loss=0.182, v_num=amm2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 202/202 [12:53<00:00,  3.83s/it, loss=0.182, v_num=amm2]\n",
      "Epoch 7: 100%|██████████| 202/202 [13:29<00:00,  4.01s/it, loss=0.182, v_num=amm2]\n",
      "Epoch 8:  99%|█████████▉| 200/202 [12:13<00:07,  3.67s/it, loss=0.181, v_num=amm2]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 202/202 [12:58<00:00,  3.86s/it, loss=0.181, v_num=amm2]\n",
      "Epoch 8: 100%|██████████| 202/202 [13:34<00:00,  4.03s/it, loss=0.181, v_num=amm2]\n",
      "Epoch 9:  99%|█████████▉| 200/202 [12:16<00:07,  3.68s/it, loss=0.18, v_num=amm2] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 202/202 [13:01<00:00,  3.87s/it, loss=0.18, v_num=amm2]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:37<00:00,  4.04s/it, loss=0.18, v_num=amm2]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:37<00:00,  4.05s/it, loss=0.18, v_num=amm2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 239912... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_pur</td><td>▁▂▄▅▆▇▇▇██</td></tr><tr><td>dist@0.8</td><td>▇▇▇█▁▄▅▆▇▆</td></tr><tr><td>eff</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▂▄▅▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>1e-05</td></tr><tr><td>cut_pur</td><td>0.00212</td></tr><tr><td>dist@0.8</td><td>0.70411</td></tr><tr><td>eff</td><td>0.7383</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00171</td></tr><tr><td>train_loss</td><td>0.17909</td></tr><tr><td>trainer/global_step</td><td>1999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">happy-sweep-4</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/j43eamm2\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/j43eamm2</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220224_232417-j43eamm2/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5zyyp5kj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.5\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/5zyyp5kj\" target=\"_blank\">expert-sweep-5</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 23.6 K\n",
      "2 | input_layer2  | Linear           | 23.6 K\n",
      "3 | layers1       | ModuleList       | 4.2 M \n",
      "4 | layers2       | ModuleList       | 4.2 M \n",
      "5 | output_layer1 | Linear           | 16.4 K\n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "8.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.5 M     Total params\n",
      "33.907    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 200/202 [12:05<00:07,  3.63s/it, loss=0.209, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 202/202 [12:51<00:00,  3.82s/it, loss=0.209, v_num=p5kj]\n",
      "Epoch 0: 100%|██████████| 202/202 [13:27<00:00,  4.00s/it, loss=0.209, v_num=p5kj]\n",
      "Epoch 1:  99%|█████████▉| 200/202 [12:02<00:07,  3.61s/it, loss=0.204, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 202/202 [12:48<00:00,  3.80s/it, loss=0.204, v_num=p5kj]\n",
      "Epoch 1: 100%|██████████| 202/202 [13:24<00:00,  3.98s/it, loss=0.204, v_num=p5kj]\n",
      "Epoch 2:  99%|█████████▉| 200/202 [13:06<00:07,  3.93s/it, loss=0.203, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 202/202 [13:52<00:00,  4.12s/it, loss=0.203, v_num=p5kj]\n",
      "Epoch 2: 100%|██████████| 202/202 [14:27<00:00,  4.29s/it, loss=0.203, v_num=p5kj]\n",
      "Epoch 3:  99%|█████████▉| 200/202 [12:41<00:07,  3.81s/it, loss=0.201, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 202/202 [13:30<00:00,  4.01s/it, loss=0.201, v_num=p5kj]\n",
      "Epoch 3: 100%|██████████| 202/202 [14:06<00:00,  4.19s/it, loss=0.201, v_num=p5kj]\n",
      "Epoch 4:  99%|█████████▉| 200/202 [13:18<00:07,  3.99s/it, loss=0.199, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 202/202 [14:05<00:00,  4.18s/it, loss=0.199, v_num=p5kj]\n",
      "Epoch 4: 100%|██████████| 202/202 [14:40<00:00,  4.36s/it, loss=0.199, v_num=p5kj]\n",
      "Epoch 5:  99%|█████████▉| 200/202 [12:07<00:07,  3.64s/it, loss=0.196, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 202/202 [12:53<00:00,  3.83s/it, loss=0.196, v_num=p5kj]\n",
      "Epoch 5: 100%|██████████| 202/202 [13:28<00:00,  4.00s/it, loss=0.196, v_num=p5kj]\n",
      "Epoch 6:  99%|█████████▉| 200/202 [12:08<00:07,  3.64s/it, loss=0.196, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 202/202 [12:54<00:00,  3.83s/it, loss=0.196, v_num=p5kj]\n",
      "Epoch 6: 100%|██████████| 202/202 [13:31<00:00,  4.02s/it, loss=0.196, v_num=p5kj]\n",
      "Epoch 7:  99%|█████████▉| 200/202 [12:09<00:07,  3.65s/it, loss=0.194, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 202/202 [12:55<00:00,  3.84s/it, loss=0.194, v_num=p5kj]\n",
      "Epoch 7: 100%|██████████| 202/202 [13:30<00:00,  4.01s/it, loss=0.194, v_num=p5kj]\n",
      "Epoch 8:  99%|█████████▉| 200/202 [12:16<00:07,  3.68s/it, loss=0.194, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: 100%|██████████| 202/202 [13:03<00:00,  3.88s/it, loss=0.194, v_num=p5kj]\n",
      "Epoch 8: 100%|██████████| 202/202 [13:40<00:00,  4.06s/it, loss=0.194, v_num=p5kj]\n",
      "Epoch 9:   0%|          | 1/202 [00:10<34:38, 10.34s/it, loss=0.194, v_num=p5kj]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Error while calling W&B API: Error 1040: Too many connections (<Response [500]>)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: Error 1040: Too many connections (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:   3%|▎         | 6/202 [00:28<15:42,  4.81s/it, loss=0.194, v_num=p5kj]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: Error 1040: Too many connections (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:   3%|▎         | 7/202 [00:32<15:11,  4.67s/it, loss=0.194, v_num=p5kj]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  16%|█▌        | 32/202 [02:03<10:54,  3.85s/it, loss=0.194, v_num=p5kj]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: Error 1040: Too many connections (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  16%|█▋        | 33/202 [02:06<10:47,  3.83s/it, loss=0.193, v_num=p5kj]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (HTTPError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  18%|█▊        | 37/202 [02:20<10:24,  3.78s/it, loss=0.194, v_num=p5kj]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: Error 1040: Too many connections (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:  99%|█████████▉| 200/202 [12:08<00:07,  3.64s/it, loss=0.193, v_num=p5kj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 202/202 [12:54<00:00,  3.83s/it, loss=0.193, v_num=p5kj]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:30<00:00,  4.01s/it, loss=0.193, v_num=p5kj]\n",
      "Epoch 9: 100%|██████████| 202/202 [13:30<00:00,  4.01s/it, loss=0.193, v_num=p5kj]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 67998... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_pur</td><td>▁▃▄▅▆█████</td></tr><tr><td>dist@0.8</td><td>▆▄▇▄▃▃▁▁▅█</td></tr><tr><td>eff</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▃▄▅▆█▇███</td></tr><tr><td>train_loss</td><td>█▄▃▃▃▃▂▂▃▂▂▂▂▂▃▂▁▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▂▁▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>1e-05</td></tr><tr><td>cut_pur</td><td>0.00213</td></tr><tr><td>dist@0.8</td><td>0.66601</td></tr><tr><td>eff</td><td>0.7383</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.0017</td></tr><tr><td>train_loss</td><td>0.19396</td></tr><tr><td>trainer/global_step</td><td>1999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">expert-sweep-5</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/5zyyp5kj\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/5zyyp5kj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220225_014122-5zyyp5kj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: t70wtwr6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.6\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.10 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/runs/t70wtwr6\" target=\"_blank\">golden-sweep-6</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_edge_embedding/sweeps/r7td01s0</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 23.6 K\n",
      "2 | input_layer2  | Linear           | 23.6 K\n",
      "3 | layers1       | ModuleList       | 4.2 M \n",
      "4 | layers2       | ModuleList       | 4.2 M \n",
      "5 | output_layer1 | Linear           | 16.4 K\n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "8.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.5 M     Total params\n",
      "33.907    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 200/202 [12:12<00:07,  3.66s/it, loss=0.219, v_num=twr6]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 202/202 [12:57<00:00,  3.85s/it, loss=0.219, v_num=twr6]\n",
      "Epoch 0: 100%|██████████| 202/202 [13:33<00:00,  4.03s/it, loss=0.219, v_num=twr6]\n",
      "Epoch 1:  99%|█████████▉| 200/202 [12:02<00:07,  3.61s/it, loss=0.213, v_num=twr6]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 202/202 [12:48<00:00,  3.80s/it, loss=0.213, v_num=twr6]\n",
      "Epoch 1: 100%|██████████| 202/202 [13:23<00:00,  3.98s/it, loss=0.213, v_num=twr6]\n",
      "Epoch 2:  82%|████████▏ | 165/202 [10:04<02:15,  3.67s/it, loss=0.212, v_num=twr6]"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_configuration, project = \"ITk_barrel_edge_embedding\")\n",
    "\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id, function=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Construct PyLightning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ML model typically has many knobs to turn, as well as locations of data, some training preferences, and so on. For convenience, let's put all of these parameters into a YAML file and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"edge_embedding_default.yaml\") as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plug these parameters into a constructor of the `LayerlessEmbedding` Lightning Module. This doesn't **do** anything yet - merely creates the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaEdgeEmbedding(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metric Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally! Let's train! We instantiate a `Trainer` class that knows things like which hardware to work with, how long to train for, and a **bunch** of default options that we ignore here. Check out the Trainer class docs in Pytorch Lightning. Suffice it to say that it clears away much repetitive boilerplate in training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='pur',\n",
    "    mode=\"max\",\n",
    "    save_top_k=2,\n",
    "    save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# kaiming_init(model)\n",
    "# logger = WandbLogger(project=\"ITk_edge_embedding\")\n",
    "# trainer = Trainer(gpus=1, max_epochs=hparams[\"max_epochs\"], logger=logger, num_sanity_val_steps=2, callbacks=[checkpoint_callback], log_every_n_steps = 10, default_root_dir=\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_embedding/\")\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
