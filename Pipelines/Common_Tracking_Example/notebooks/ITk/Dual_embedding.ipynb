{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " pretrained_weight_and_margin\n"
     ]
    }
   ],
   "source": [
    "run_name = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# External imports\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import frnn\n",
    "import wandb\n",
    "import math\n",
    "sys.path.append('../..')\n",
    "\n",
    "from LightningModules.DualEmbedding.Models.vanilla_dual_embedding import VanillaDualEmbedding\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        elif name.startswith(\"layers.0\"):  # The first layer does not have ReLU applied on its input\n",
    "            param.data.normal_(0, 1 / math.sqrt(param.shape[1]))\n",
    "        else:\n",
    "            param.data.normal_(0, math.sqrt(2) / math.sqrt(param.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dual_embedding_sweep.yaml\") as f:\n",
    "        sweep_hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "with open(\"dual_embedding_default.yaml\") as f:\n",
    "        default_hparams = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    \"name\": run_name,\n",
    "    \"project\": \"ITk_barrell_dual_embedding\",\n",
    "    \"metric\": {\"name\": \"pur\", \"goal\": \"maximize\"},\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": sweep_hparams\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(model):\n",
    "    checkpoint = torch.load(\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_embedding/ITk_dual_embedding/3ijb4qnw/checkpoints/last.ckpt\")\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "    names = [i for i in state_dict]\n",
    "    for i in names:\n",
    "        state = state_dict[i]\n",
    "        i = i.replace(\"input_layer1\", \"input_layer2\")\n",
    "        i = i.replace(\"layers1\", \"layers2\")\n",
    "        i = i.replace(\"output_layer1\", \"output_layer2\")\n",
    "        state_dict[i] = state\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    del state_dict\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    wandb.init()\n",
    "    model = VanillaDualEmbedding({**default_hparams, **wandb.config})\n",
    "    # kaiming_init(model)\n",
    "    model = load_dict(model)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='pur',\n",
    "        mode=\"max\",\n",
    "        save_top_k=2,\n",
    "        save_last=True)\n",
    "\n",
    "    logger = WandbLogger()\n",
    "    trainer = Trainer(gpus=1, max_epochs=default_hparams[\"max_epochs\"], log_every_n_steps = 50, logger=logger, callbacks=[checkpoint_callback], default_root_dir=\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_dual_embedding/\")\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 9ycd43iv\n",
      "Sweep URL: https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vfydayb3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.5\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mexatrkx\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/vfydayb3\" target=\"_blank\">magic-sweep-1</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:02<00:02,  2.58s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:36<00:01,  3.62it/s, loss=0.217, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:45<00:00,  3.51it/s, loss=0.217, v_num=ayb3]\n",
      "Epoch 0: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.39it/s, loss=0.217, v_num=ayb3]\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [05:01<00:00,  3.33it/s, loss=0.217, v_num=ayb3]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:09<00:00,  3.25it/s, loss=0.217, v_num=ayb3]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:19<00:00,  3.15it/s, loss=0.217, v_num=ayb3]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.215, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 1: 100%|█████████▉| 1003/1005 [05:05<00:00,  3.28it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.28it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:13<00:00,  3.21it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:22<00:00,  3.11it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.215, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [05:02<00:00,  3.32it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.29it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:13<00:00,  3.21it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:22<00:00,  3.12it/s, loss=0.215, v_num=ayb3]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.214, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.214, v_num=ayb3]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.214, v_num=ayb3]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.214, v_num=ayb3]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:15<00:00,  3.19it/s, loss=0.214, v_num=ayb3]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.214, v_num=ayb3]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:40<00:01,  3.57it/s, loss=0.213, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:50<00:00,  3.45it/s, loss=0.213, v_num=ayb3]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [04:59<00:00,  3.35it/s, loss=0.213, v_num=ayb3]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.25it/s, loss=0.213, v_num=ayb3]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:17<00:00,  3.17it/s, loss=0.213, v_num=ayb3]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.213, v_num=ayb3]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.57it/s, loss=0.212, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.212, v_num=ayb3]\n",
      "Validating:  40%|████      | 2/5 [00:15<00:23,  7.77s/it]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:03<00:00,  3.30it/s, loss=0.212, v_num=ayb3]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:10<00:00,  3.23it/s, loss=0.212, v_num=ayb3]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:19<00:00,  3.15it/s, loss=0.212, v_num=ayb3]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.212, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.50it/s, loss=0.212, v_num=ayb3]\n",
      "Epoch 6: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.38it/s, loss=0.212, v_num=ayb3]\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [05:01<00:00,  3.33it/s, loss=0.212, v_num=ayb3]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:07<00:00,  3.27it/s, loss=0.212, v_num=ayb3]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:15<00:00,  3.19it/s, loss=0.212, v_num=ayb3]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.211, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.50it/s, loss=0.211, v_num=ayb3]\n",
      "Validating:  40%|████      | 2/5 [00:14<00:20,  6.92s/it]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [05:00<00:00,  3.35it/s, loss=0.211, v_num=ayb3]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:06<00:00,  3.28it/s, loss=0.211, v_num=ayb3]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:14<00:00,  3.20it/s, loss=0.211, v_num=ayb3]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.211, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.49it/s, loss=0.211, v_num=ayb3]\n",
      "Validating:  40%|████      | 2/5 [00:13<00:20,  6.81s/it]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [05:00<00:00,  3.34it/s, loss=0.211, v_num=ayb3]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:06<00:00,  3.28it/s, loss=0.211, v_num=ayb3]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:14<00:00,  3.20it/s, loss=0.211, v_num=ayb3]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.211, v_num=ayb3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.50it/s, loss=0.211, v_num=ayb3]\n",
      "Validating:  40%|████      | 2/5 [00:13<00:19,  6.55s/it]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [04:59<00:00,  3.35it/s, loss=0.211, v_num=ayb3]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:05<00:00,  3.29it/s, loss=0.211, v_num=ayb3]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:12<00:00,  3.21it/s, loss=0.211, v_num=ayb3]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:13<00:00,  3.21it/s, loss=0.211, v_num=ayb3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 255806... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▄▃▆▁▁▆▇▇██</td></tr><tr><td>cut_pur</td><td>▁▂▃▃▄▅▆▇▇█</td></tr><tr><td>dist@0.98</td><td>▁▂▃▅█▇▇█▇█</td></tr><tr><td>eff</td><td>█▇▆▅▄▃▂▂▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▂▃▃▄▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▆▄▄▄▃▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▂▁▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97667</td></tr><tr><td>cut_pur</td><td>0.00019</td></tr><tr><td>dist@0.98</td><td>0.42055</td></tr><tr><td>eff</td><td>0.87519</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00109</td></tr><tr><td>train_loss</td><td>0.21014</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">magic-sweep-1</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/vfydayb3\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/vfydayb3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220309_231654-vfydayb3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xe4j70hs with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.75\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/xe4j70hs\" target=\"_blank\">good-sweep-2</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:36<00:01,  3.61it/s, loss=0.238, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.49it/s, loss=0.238, v_num=70hs]\n",
      "Epoch 0: 100%|█████████▉| 1003/1005 [05:03<00:00,  3.31it/s, loss=0.238, v_num=70hs]\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [05:03<00:00,  3.31it/s, loss=0.238, v_num=70hs]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:11<00:00,  3.22it/s, loss=0.238, v_num=70hs]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:21<00:00,  3.13it/s, loss=0.238, v_num=70hs]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.61it/s, loss=0.236, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.236, v_num=70hs]\n",
      "Epoch 1: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.38it/s, loss=0.236, v_num=70hs]\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.28it/s, loss=0.236, v_num=70hs]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:14<00:00,  3.19it/s, loss=0.236, v_num=70hs]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:24<00:00,  3.10it/s, loss=0.236, v_num=70hs]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.60it/s, loss=0.235, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.235, v_num=70hs]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.235, v_num=70hs]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.235, v_num=70hs]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.235, v_num=70hs]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.235, v_num=70hs]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.60it/s, loss=0.234, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.26it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:17<00:00,  3.17it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:27<00:00,  3.07it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.57it/s, loss=0.234, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:50<00:00,  3.45it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [04:59<00:00,  3.35it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:09<00:00,  3.24it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:18<00:00,  3.16it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:28<00:00,  3.06it/s, loss=0.234, v_num=70hs]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:40<00:01,  3.57it/s, loss=0.232, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:50<00:00,  3.45it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 5: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.26it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:14<00:00,  3.20it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:23<00:00,  3.11it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.57it/s, loss=0.232, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 6: 100%|█████████▉| 1003/1005 [04:59<00:00,  3.35it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [05:04<00:00,  3.29it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:11<00:00,  3.22it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:20<00:00,  3.14it/s, loss=0.232, v_num=70hs]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.231, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 7: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [05:03<00:00,  3.31it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:09<00:00,  3.24it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:17<00:00,  3.16it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.231, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 8: 100%|█████████▉| 1003/1005 [05:00<00:00,  3.34it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [05:02<00:00,  3.32it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:08<00:00,  3.26it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.231, v_num=70hs]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.231, v_num=70hs]\n",
      "Validating:  40%|████      | 2/5 [00:15<00:22,  7.45s/it]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [05:01<00:00,  3.32it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:08<00:00,  3.26it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.231, v_num=70hs]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:17<00:00,  3.17it/s, loss=0.231, v_num=70hs]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 35119... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▇▄▂▁▂▅▅▆▆█</td></tr><tr><td>cut_pur</td><td>▁▂▂▃▃▅▆▇▇█</td></tr><tr><td>dist@0.98</td><td>▁▄▅█▆▆▆▇▇▇</td></tr><tr><td>eff</td><td>█▆▆▃▄▃▃▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▂▂▃▃▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▇▅▅▄▄▄▃▄▃▃▃▃▃▂▃▃▃▃▃▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97657</td></tr><tr><td>cut_pur</td><td>0.00019</td></tr><tr><td>dist@0.98</td><td>0.38552</td></tr><tr><td>eff</td><td>0.87581</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00108</td></tr><tr><td>train_loss</td><td>0.22926</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">good-sweep-2</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/xe4j70hs\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/xe4j70hs</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_001037-xe4j70hs/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5e9czlgy with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/5e9czlgy\" target=\"_blank\">usual-sweep-3</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.61it/s, loss=0.242, v_num=zlgy]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.49it/s, loss=0.242, v_num=zlgy]\n",
      "Validating:  40%|████      | 2/5 [00:17<00:25,  8.48s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [05:03<00:00,  3.31it/s, loss=0.242, v_num=zlgy]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:11<00:00,  3.23it/s, loss=0.242, v_num=zlgy]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:20<00:00,  3.14it/s, loss=0.242, v_num=zlgy]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.60it/s, loss=0.24, v_num=zlgy] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.24, v_num=zlgy]\n",
      "Epoch 1: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.24, v_num=zlgy]\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.24, v_num=zlgy]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.24, v_num=zlgy]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.24, v_num=zlgy]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.239, v_num=zlgy]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.239, v_num=zlgy]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.239, v_num=zlgy]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.25it/s, loss=0.239, v_num=zlgy]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:17<00:00,  3.17it/s, loss=0.239, v_num=zlgy]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:27<00:00,  3.07it/s, loss=0.239, v_num=zlgy]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.238, v_num=zlgy]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.38it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.27it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.238, v_num=zlgy]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.49it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [05:04<00:00,  3.30it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.28it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:15<00:00,  3.19it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:25<00:00,  3.08it/s, loss=0.238, v_num=zlgy]\n",
      "Epoch 5:  29%|██▉       | 292/1005 [01:21<03:19,  3.58it/s, loss=0.236, v_num=zlgy] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error while calling W&B API: context deadline exceeded (<Response [500]>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.236, v_num=zlgy]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.236, v_num=zlgy]\n",
      "Validating:  40%|████      | 2/5 [00:17<00:25,  8.60s/it]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.28it/s, loss=0.236, v_num=zlgy]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:13<00:00,  3.21it/s, loss=0.236, v_num=zlgy]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:22<00:00,  3.12it/s, loss=0.236, v_num=zlgy]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.236, v_num=zlgy]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.236, v_num=zlgy]\n",
      "Epoch 6: 100%|█████████▉| 1003/1005 [05:04<00:00,  3.29it/s, loss=0.236, v_num=zlgy]\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.29it/s, loss=0.236, v_num=zlgy]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:12<00:00,  3.22it/s, loss=0.236, v_num=zlgy]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:21<00:00,  3.13it/s, loss=0.236, v_num=zlgy]\n",
      "Epoch 7:  11%|█         | 108/1005 [00:30<04:12,  3.56it/s, loss=0.236, v_num=zlgy] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.235, v_num=zlgy]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 7: 100%|█████████▉| 1003/1005 [05:02<00:00,  3.31it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [05:03<00:00,  3.31it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:10<00:00,  3.23it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:19<00:00,  3.15it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:40<00:01,  3.57it/s, loss=0.235, v_num=zlgy]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.235, v_num=zlgy]\n",
      "Validating:  40%|████      | 2/5 [00:14<00:21,  7.28s/it]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [05:02<00:00,  3.32it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:09<00:00,  3.25it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:17<00:00,  3.17it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.235, v_num=zlgy]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.49it/s, loss=0.235, v_num=zlgy]\n",
      "Validating:  40%|████      | 2/5 [00:14<00:21,  7.08s/it]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [05:01<00:00,  3.33it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:07<00:00,  3.26it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.235, v_num=zlgy]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.235, v_num=zlgy]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 74607... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▆▂▁▂▃▄▅▇▇█</td></tr><tr><td>cut_pur</td><td>▁▁▂▃▃▅▆▇▇█</td></tr><tr><td>dist@0.98</td><td>▁▅▆▆▇▇▇▇██</td></tr><tr><td>eff</td><td>█▇▆▅▅▃▂▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▁▂▃▃▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▄▃▃▃▃▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97652</td></tr><tr><td>cut_pur</td><td>0.00019</td></tr><tr><td>dist@0.98</td><td>0.36243</td></tr><tr><td>eff</td><td>0.87676</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00106</td></tr><tr><td>train_loss</td><td>0.23369</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">usual-sweep-3</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/5e9czlgy\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/5e9czlgy</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_010437-5e9czlgy/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o18o93dm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 1.5\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/o18o93dm\" target=\"_blank\">stilted-sweep-4</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:36<00:01,  3.62it/s, loss=0.232, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:45<00:00,  3.51it/s, loss=0.232, v_num=93dm]\n",
      "Epoch 0: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.39it/s, loss=0.232, v_num=93dm]\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [05:02<00:00,  3.31it/s, loss=0.232, v_num=93dm]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:11<00:00,  3.23it/s, loss=0.232, v_num=93dm]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:20<00:00,  3.13it/s, loss=0.232, v_num=93dm]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.231, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.231, v_num=93dm]\n",
      "Epoch 1: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.231, v_num=93dm]\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.26it/s, loss=0.231, v_num=93dm]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:16<00:00,  3.17it/s, loss=0.231, v_num=93dm]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:27<00:00,  3.07it/s, loss=0.231, v_num=93dm]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.229, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.26it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:16<00:00,  3.17it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:26<00:00,  3.07it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.229, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:59<00:00,  3.35it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:09<00:00,  3.24it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:19<00:00,  3.15it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:29<00:00,  3.05it/s, loss=0.229, v_num=93dm]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.228, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.228, v_num=93dm]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.228, v_num=93dm]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.228, v_num=93dm]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.228, v_num=93dm]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.228, v_num=93dm]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.227, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 5: 100%|█████████▉| 1003/1005 [05:03<00:00,  3.31it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.28it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:13<00:00,  3.20it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:23<00:00,  3.10it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:40<00:01,  3.56it/s, loss=0.227, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.227, v_num=93dm]\n",
      "Validating:  40%|████      | 2/5 [00:16<00:24,  8.15s/it]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.28it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:14<00:00,  3.20it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:23<00:00,  3.11it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:36<00:01,  3.62it/s, loss=0.227, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:45<00:00,  3.52it/s, loss=0.227, v_num=93dm]\n",
      "Validating:  40%|████      | 2/5 [00:15<00:23,  7.73s/it]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [05:00<00:00,  3.34it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:07<00:00,  3.27it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:16<00:00,  3.17it/s, loss=0.227, v_num=93dm]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:51<00:01,  3.43it/s, loss=0.226, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:59<00:00,  3.35it/s, loss=0.226, v_num=93dm]\n",
      "Validating:  40%|████      | 2/5 [00:15<00:23,  7.82s/it]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [05:15<00:00,  3.19it/s, loss=0.226, v_num=93dm]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:21<00:00,  3.12it/s, loss=0.226, v_num=93dm]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:30<00:00,  3.05it/s, loss=0.226, v_num=93dm]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [05:09<00:01,  3.23it/s, loss=0.226, v_num=93dm]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [05:17<00:00,  3.16it/s, loss=0.226, v_num=93dm]\n",
      "Validating:  40%|████      | 2/5 [00:15<00:22,  7.61s/it]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [05:32<00:00,  3.02it/s, loss=0.226, v_num=93dm]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:39<00:00,  2.96it/s, loss=0.226, v_num=93dm]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:47<00:00,  2.90it/s, loss=0.226, v_num=93dm]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:47<00:00,  2.89it/s, loss=0.226, v_num=93dm]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 112705... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▆▄▁▄▂▆▆▇██</td></tr><tr><td>cut_pur</td><td>▁▁▂▃▄▅▆▆██</td></tr><tr><td>dist@0.98</td><td>▁▄▃▅▅▆▆▇██</td></tr><tr><td>eff</td><td>█▇▆▅▃▃▃▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▁▂▃▄▅▆▆██</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▄▄▄▃▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▃▁▁▂▂▂▁▂▂▂▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97642</td></tr><tr><td>cut_pur</td><td>0.00018</td></tr><tr><td>dist@0.98</td><td>0.31782</td></tr><tr><td>eff</td><td>0.87971</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00103</td></tr><tr><td>train_loss</td><td>0.22485</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">stilted-sweep-4</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/o18o93dm\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/o18o93dm</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_015836-o18o93dm/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m1sl7avx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/m1sl7avx\" target=\"_blank\">fresh-sweep-5</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.216, v_num=7avx]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.47it/s, loss=0.216, v_num=7avx]\n",
      "Epoch 0: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.216, v_num=7avx]\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.28it/s, loss=0.216, v_num=7avx]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:13<00:00,  3.20it/s, loss=0.216, v_num=7avx]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:22<00:00,  3.11it/s, loss=0.216, v_num=7avx]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.214, v_num=7avx]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.214, v_num=7avx]\n",
      "Epoch 1: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.214, v_num=7avx]\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.26it/s, loss=0.214, v_num=7avx]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.214, v_num=7avx]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.214, v_num=7avx]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.213, v_num=7avx]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:50<00:00,  3.45it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [04:59<00:00,  3.35it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:09<00:00,  3.25it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:17<00:00,  3.16it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:27<00:00,  3.06it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.213, v_num=7avx]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:59<00:00,  3.35it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:10<00:00,  3.24it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:19<00:00,  3.15it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:31<00:00,  3.04it/s, loss=0.213, v_num=7avx]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.212, v_num=7avx]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.212, v_num=7avx]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.212, v_num=7avx]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:09<00:00,  3.25it/s, loss=0.212, v_num=7avx]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:18<00:00,  3.16it/s, loss=0.212, v_num=7avx]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:29<00:00,  3.05it/s, loss=0.212, v_num=7avx]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.61it/s, loss=0.211, v_num=7avx]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 5: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.38it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.27it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:15<00:00,  3.19it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.211, v_num=7avx]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.211, v_num=7avx]\n",
      "Validating:  40%|████      | 2/5 [00:18<00:27,  9.14s/it]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:24<00:00,  3.09it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.21, v_num=7avx] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.21, v_num=7avx]\n",
      "Epoch 7: 100%|█████████▉| 1003/1005 [05:00<00:00,  3.33it/s, loss=0.21, v_num=7avx]\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.29it/s, loss=0.21, v_num=7avx]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:13<00:00,  3.21it/s, loss=0.21, v_num=7avx]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:22<00:00,  3.12it/s, loss=0.21, v_num=7avx]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.211, v_num=7avx]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.49it/s, loss=0.211, v_num=7avx]\n",
      "Validating:  40%|████      | 2/5 [00:14<00:22,  7.42s/it]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [05:01<00:00,  3.33it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:08<00:00,  3.26it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:16<00:00,  3.17it/s, loss=0.211, v_num=7avx]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.21, v_num=7avx] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.50it/s, loss=0.21, v_num=7avx]\n",
      "Validating:  40%|████      | 2/5 [00:15<00:22,  7.58s/it]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [05:00<00:00,  3.34it/s, loss=0.21, v_num=7avx]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:07<00:00,  3.27it/s, loss=0.21, v_num=7avx]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.21, v_num=7avx]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:18<00:00,  3.16it/s, loss=0.21, v_num=7avx]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 151426... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▆▆▁▃▁▅▇▇██</td></tr><tr><td>cut_pur</td><td>▁▁▂▂▃▅▆▆██</td></tr><tr><td>dist@0.98</td><td>▁▄▆▅█▇██▇█</td></tr><tr><td>eff</td><td>▇█▆▅▄▃▃▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▁▂▂▃▅▆▆██</td></tr><tr><td>train_loss</td><td>█▇▄▅▄▄▃▃▄▃▃▃▃▃▃▃▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97626</td></tr><tr><td>cut_pur</td><td>0.00018</td></tr><tr><td>dist@0.98</td><td>0.27898</td></tr><tr><td>eff</td><td>0.88107</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00101</td></tr><tr><td>train_loss</td><td>0.2101</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fresh-sweep-5</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/m1sl7avx\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/m1sl7avx</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_025334-m1sl7avx/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: lc0p2s9p with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 3\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/lc0p2s9p\" target=\"_blank\">absurd-sweep-6</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.184, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.50it/s, loss=0.184, v_num=2s9p]\n",
      "Validating:  40%|████      | 2/5 [00:16<00:23,  7.91s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [05:01<00:00,  3.33it/s, loss=0.184, v_num=2s9p]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:09<00:00,  3.25it/s, loss=0.184, v_num=2s9p]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:17<00:00,  3.16it/s, loss=0.184, v_num=2s9p]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.181, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 1: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.38it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:15<00:00,  3.19it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.181, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [05:04<00:00,  3.30it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.27it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:15<00:00,  3.19it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.181, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.181, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.26it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:18<00:00,  3.16it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:29<00:00,  3.05it/s, loss=0.181, v_num=2s9p]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.179, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 5: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.37it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.26it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:16<00:00,  3.17it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:27<00:00,  3.07it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.179, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 6: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.26it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.179, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 7: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.39it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.29it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:13<00:00,  3.21it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:22<00:00,  3.11it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.179, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 8: 100%|█████████▉| 1003/1005 [05:05<00:00,  3.28it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.28it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:14<00:00,  3.20it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:23<00:00,  3.10it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.179, v_num=2s9p]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 9: 100%|█████████▉| 1003/1005 [05:00<00:00,  3.33it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.28it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:13<00:00,  3.21it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:22<00:00,  3.11it/s, loss=0.179, v_num=2s9p]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:23<00:00,  3.11it/s, loss=0.179, v_num=2s9p]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 190287... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▃▅▁▃▂▆▆▆▇█</td></tr><tr><td>cut_pur</td><td>▁▁▂▃▂▅▆▆██</td></tr><tr><td>dist@0.98</td><td>▁▅▄▆▇▇██▇█</td></tr><tr><td>eff</td><td>▇█▄▂▅▃▃▂▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▁▂▂▂▅▆▆██</td></tr><tr><td>train_loss</td><td>█▆▄▅▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▁▂▂▂▂▁▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97634</td></tr><tr><td>cut_pur</td><td>0.00017</td></tr><tr><td>dist@0.98</td><td>0.22484</td></tr><tr><td>eff</td><td>0.88324</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00097</td></tr><tr><td>train_loss</td><td>0.17962</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">absurd-sweep-6</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/lc0p2s9p\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/lc0p2s9p</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_034818-lc0p2s9p/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pqeldq4t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 4\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/pqeldq4t\" target=\"_blank\">skilled-sweep-7</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.156, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.49it/s, loss=0.156, v_num=dq4t]\n",
      "Epoch 0: 100%|█████████▉| 1003/1005 [05:02<00:00,  3.32it/s, loss=0.156, v_num=dq4t]\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [05:02<00:00,  3.31it/s, loss=0.156, v_num=dq4t]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:09<00:00,  3.24it/s, loss=0.156, v_num=dq4t]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:18<00:00,  3.16it/s, loss=0.156, v_num=dq4t]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.58it/s, loss=0.155, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.47it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 1: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.26it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.155, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.26it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.155, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:59<00:00,  3.35it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:09<00:00,  3.24it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:19<00:00,  3.15it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:29<00:00,  3.05it/s, loss=0.155, v_num=dq4t]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.154, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.25it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:17<00:00,  3.17it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:27<00:00,  3.07it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.154, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 5: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.25it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:16<00:00,  3.17it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:27<00:00,  3.07it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.154, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 6: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.38it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:15<00:00,  3.19it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.154, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 7: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.39it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.28it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:13<00:00,  3.20it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:23<00:00,  3.10it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.154, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.154, v_num=dq4t]\n",
      "Validating:  40%|████      | 2/5 [00:18<00:26,  8.92s/it]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.28it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:15<00:00,  3.19it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.154, v_num=dq4t]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 9: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.38it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.28it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:13<00:00,  3.20it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:23<00:00,  3.11it/s, loss=0.154, v_num=dq4t]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:23<00:00,  3.10it/s, loss=0.154, v_num=dq4t]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 229148... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▅▆▃▁▃▆▇▇█▇</td></tr><tr><td>cut_pur</td><td>▂▁▁▁▃▅▆▆▇█</td></tr><tr><td>dist@0.98</td><td>▁▄▆█▇▇▇██▇</td></tr><tr><td>eff</td><td>▆█▅▄▁▄▂▂▁▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▂▁▁▁▃▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▅▅▄▃▃▃▄▂▃▃▂▃▂▂▃▂▂▃▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁▂▂▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97622</td></tr><tr><td>cut_pur</td><td>0.00016</td></tr><tr><td>dist@0.98</td><td>0.18876</td></tr><tr><td>eff</td><td>0.88591</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00094</td></tr><tr><td>train_loss</td><td>0.15269</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">skilled-sweep-7</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/pqeldq4t\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/pqeldq4t</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_044340-pqeldq4t/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dpw2rrhi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 6\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/dpw2rrhi\" target=\"_blank\">effortless-sweep-8</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.60it/s, loss=0.121, v_num=rrhi]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.50it/s, loss=0.121, v_num=rrhi]\n",
      "Validating:  40%|████      | 2/5 [00:15<00:22,  7.52s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [05:01<00:00,  3.33it/s, loss=0.121, v_num=rrhi]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:08<00:00,  3.26it/s, loss=0.121, v_num=rrhi]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:17<00:00,  3.17it/s, loss=0.121, v_num=rrhi]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.61it/s, loss=0.12, v_num=rrhi] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.49it/s, loss=0.12, v_num=rrhi]\n",
      "Epoch 1: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.39it/s, loss=0.12, v_num=rrhi]\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.28it/s, loss=0.12, v_num=rrhi]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:14<00:00,  3.20it/s, loss=0.12, v_num=rrhi]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:24<00:00,  3.10it/s, loss=0.12, v_num=rrhi]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.119, v_num=rrhi]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.38it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.27it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:15<00:00,  3.19it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.119, v_num=rrhi]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.38it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:25<00:00,  3.08it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.119, v_num=rrhi]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.38it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.27it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.119, v_num=rrhi]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.49it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 5: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.39it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.28it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:14<00:00,  3.20it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:24<00:00,  3.10it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.119, v_num=rrhi]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 6: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.38it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:36<00:01,  3.61it/s, loss=0.119, v_num=rrhi]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.49it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 7: 100%|█████████▉| 1003/1005 [05:02<00:00,  3.32it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.29it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:13<00:00,  3.21it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:23<00:00,  3.11it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.60it/s, loss=0.119, v_num=rrhi]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 8: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.28it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:13<00:00,  3.20it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:22<00:00,  3.11it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.61it/s, loss=0.119, v_num=rrhi]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 9: 100%|█████████▉| 1003/1005 [04:55<00:00,  3.39it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [05:04<00:00,  3.29it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:13<00:00,  3.21it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:23<00:00,  3.11it/s, loss=0.119, v_num=rrhi]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:24<00:00,  3.10it/s, loss=0.119, v_num=rrhi]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 8116... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▆▆▆▁▄▆█▇▇█</td></tr><tr><td>cut_pur</td><td>▄▁▂▂▃▄▅▇▇█</td></tr><tr><td>dist@0.98</td><td>▁▄▅▆▇██▇██</td></tr><tr><td>eff</td><td>▆█▆▅▃▂▃▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▄▁▂▂▃▄▅▇▇█</td></tr><tr><td>train_loss</td><td>█▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97631</td></tr><tr><td>cut_pur</td><td>0.00016</td></tr><tr><td>dist@0.98</td><td>0.14289</td></tr><tr><td>eff</td><td>0.88812</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00091</td></tr><tr><td>train_loss</td><td>0.11874</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">effortless-sweep-8</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/dpw2rrhi\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/dpw2rrhi</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_053849-dpw2rrhi/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e8cckta3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 8\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/e8cckta3\" target=\"_blank\">fanciful-sweep-9</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.60it/s, loss=0.0976, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:46<00:00,  3.50it/s, loss=0.0976, v_num=kta3]\n",
      "Validating:  40%|████      | 2/5 [00:15<00:23,  7.82s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [05:02<00:00,  3.32it/s, loss=0.0976, v_num=kta3]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:09<00:00,  3.25it/s, loss=0.0976, v_num=kta3]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:17<00:00,  3.16it/s, loss=0.0976, v_num=kta3]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.0971, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.0971, v_num=kta3]\n",
      "Epoch 1: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.0971, v_num=kta3]\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.0971, v_num=kta3]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.0971, v_num=kta3]\n",
      "Epoch 1: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.0971, v_num=kta3]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.0969, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.0969, v_num=kta3]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.38it/s, loss=0.0969, v_num=kta3]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:06<00:00,  3.28it/s, loss=0.0969, v_num=kta3]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:14<00:00,  3.19it/s, loss=0.0969, v_num=kta3]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.0969, v_num=kta3]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.61it/s, loss=0.0968, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.48it/s, loss=0.0968, v_num=kta3]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.38it/s, loss=0.0968, v_num=kta3]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.0968, v_num=kta3]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:16<00:00,  3.17it/s, loss=0.0968, v_num=kta3]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:26<00:00,  3.07it/s, loss=0.0968, v_num=kta3]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.0967, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.0967, v_num=kta3]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [04:59<00:00,  3.35it/s, loss=0.0967, v_num=kta3]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:09<00:00,  3.24it/s, loss=0.0967, v_num=kta3]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:18<00:00,  3.15it/s, loss=0.0967, v_num=kta3]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:28<00:00,  3.06it/s, loss=0.0967, v_num=kta3]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:37<00:01,  3.60it/s, loss=0.0965, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.48it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 5: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.26it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:16<00:00,  3.17it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:26<00:00,  3.07it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.0964, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 6: 100%|█████████▉| 1003/1005 [04:57<00:00,  3.37it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 6: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.0965, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 7: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.37it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.26it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 7: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.0965, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.46it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 8: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.26it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:16<00:00,  3.17it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 8: 100%|██████████| 1005/1005 [05:26<00:00,  3.08it/s, loss=0.0965, v_num=kta3]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.0964, v_num=kta3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:48<00:00,  3.47it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 9: 100%|█████████▉| 1003/1005 [05:02<00:00,  3.31it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [05:07<00:00,  3.27it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:15<00:00,  3.18it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.0964, v_num=kta3]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [05:25<00:00,  3.09it/s, loss=0.0964, v_num=kta3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 47095... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▁▄▇▅▆▇█▇██</td></tr><tr><td>cut_pur</td><td>▃▁▄▃▄▅▇▇▇█</td></tr><tr><td>dist@0.98</td><td>▁▄▄▇▇█▇▇██</td></tr><tr><td>eff</td><td>█▆▃▂▂▃▂▂▃▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▄▁▄▂▄▅▇▇▇█</td></tr><tr><td>train_loss</td><td>█▃█▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▁▁▂▁▁▁▂▁▁▁▁▁▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97635</td></tr><tr><td>cut_pur</td><td>0.00015</td></tr><tr><td>dist@0.98</td><td>0.11462</td></tr><tr><td>eff</td><td>0.88869</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00089</td></tr><tr><td>train_loss</td><td>0.09634</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">fanciful-sweep-9</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/e8cckta3\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/e8cckta3</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_063307-e8cckta3/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: sd2wuajj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.5\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/sd2wuajj\" target=\"_blank\">astral-sweep-10</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:28<00:01,  3.73it/s, loss=0.204, v_num=uajj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:31<00:00,  3.69it/s, loss=0.204, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:05<00:07,  2.53s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [04:36<00:00,  3.64it/s, loss=0.204, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:09<00:02,  2.27s/it]\u001b[A\n",
      "Epoch 0: 100%|██████████| 1005/1005 [04:40<00:00,  3.58it/s, loss=0.204, v_num=uajj]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:27<00:01,  3.75it/s, loss=0.198, v_num=uajj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:30<00:00,  3.71it/s, loss=0.198, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:06<00:09,  3.02s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [04:35<00:00,  3.64it/s, loss=0.198, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:11<00:02,  2.92s/it]\u001b[A\n",
      "Epoch 1: 100%|██████████| 1005/1005 [04:42<00:00,  3.56it/s, loss=0.198, v_num=uajj]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:27<00:01,  3.74it/s, loss=0.197, v_num=uajj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:30<00:00,  3.71it/s, loss=0.197, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:04<00:06,  2.29s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [04:34<00:00,  3.66it/s, loss=0.197, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:08<00:02,  2.07s/it]\u001b[A\n",
      "Epoch 2: 100%|██████████| 1005/1005 [04:38<00:00,  3.60it/s, loss=0.197, v_num=uajj]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:26<00:01,  3.75it/s, loss=0.196, v_num=uajj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:29<00:00,  3.71it/s, loss=0.196, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:04<00:07,  2.38s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [04:34<00:00,  3.66it/s, loss=0.196, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:09<00:02,  2.13s/it]\u001b[A\n",
      "Epoch 3: 100%|██████████| 1005/1005 [04:38<00:00,  3.61it/s, loss=0.196, v_num=uajj]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:26<00:01,  3.75it/s, loss=0.21, v_num=uajj] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:30<00:00,  3.71it/s, loss=0.21, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:05<00:08,  2.73s/it]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [04:35<00:00,  3.65it/s, loss=0.21, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:10<00:02,  2.47s/it]\u001b[A\n",
      "Epoch 4: 100%|██████████| 1005/1005 [04:40<00:00,  3.58it/s, loss=0.21, v_num=uajj]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:24<00:01,  3.78it/s, loss=0.183, v_num=uajj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:26<00:00,  3.76it/s, loss=0.183, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:03<00:04,  1.61s/it]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [04:29<00:00,  3.72it/s, loss=0.183, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:06<00:01,  1.43s/it]\u001b[A\n",
      "Epoch 5: 100%|██████████| 1005/1005 [04:32<00:00,  3.69it/s, loss=0.183, v_num=uajj]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:23<00:01,  3.80it/s, loss=0.183, v_num=uajj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:25<00:00,  3.78it/s, loss=0.183, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.28s/it]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [04:27<00:00,  3.76it/s, loss=0.183, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:05<00:01,  1.18s/it]\u001b[A\n",
      "Epoch 6: 100%|██████████| 1005/1005 [04:29<00:00,  3.72it/s, loss=0.183, v_num=uajj]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:25<00:01,  3.76it/s, loss=0.18, v_num=uajj] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:27<00:00,  3.75it/s, loss=0.18, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.18s/it]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [04:29<00:00,  3.73it/s, loss=0.18, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:04<00:00,  1.04it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 1005/1005 [04:31<00:00,  3.71it/s, loss=0.18, v_num=uajj]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:23<00:01,  3.79it/s, loss=0.181, v_num=uajj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:25<00:00,  3.78it/s, loss=0.181, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.06s/it]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [04:26<00:00,  3.76it/s, loss=0.181, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:03<00:00,  1.18it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 1005/1005 [04:28<00:00,  3.74it/s, loss=0.181, v_num=uajj]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:24<00:01,  3.78it/s, loss=0.182, v_num=uajj]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:26<00:00,  3.77it/s, loss=0.182, v_num=uajj]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.02s/it]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [04:27<00:00,  3.75it/s, loss=0.182, v_num=uajj]\n",
      "Validating:  80%|████████  | 4/5 [00:03<00:00,  1.23it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 1005/1005 [04:29<00:00,  3.73it/s, loss=0.182, v_num=uajj]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [04:29<00:00,  3.72it/s, loss=0.182, v_num=uajj]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 87416... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▆▁▇▇▆▆█▇▇▆</td></tr><tr><td>cut_pur</td><td>▂▁▂▂▁▄▆▇██</td></tr><tr><td>dist@0.98</td><td>▃█▁▆▁▆▆▆▆▇</td></tr><tr><td>eff</td><td>██▇▆▇▄▃▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▂▁▂▂▁▄▆▇██</td></tr><tr><td>train_loss</td><td>▇▆▅▆▇▆▅▅▅█▄▅▄▄▄▅▄▅▄▃▃▃▃▂▃▂▂▃▃▁▂▃▃▂▃▂▂▂▃▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97764</td></tr><tr><td>cut_pur</td><td>0.00048</td></tr><tr><td>dist@0.98</td><td>0.4529</td></tr><tr><td>eff</td><td>0.86513</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00269</td></tr><tr><td>train_loss</td><td>0.17601</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">astral-sweep-10</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/sd2wuajj\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/sd2wuajj</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_072740-sd2wuajj/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8il3bnpo with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 0.75\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/8il3bnpo\" target=\"_blank\">still-sweep-11</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:29<00:01,  3.71it/s, loss=0.233, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:37<00:00,  3.61it/s, loss=0.233, v_num=bnpo]\n",
      "Epoch 0: 100%|█████████▉| 1003/1005 [04:50<00:00,  3.45it/s, loss=0.233, v_num=bnpo]\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [04:52<00:00,  3.44it/s, loss=0.233, v_num=bnpo]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [04:58<00:00,  3.37it/s, loss=0.233, v_num=bnpo]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:06<00:00,  3.28it/s, loss=0.233, v_num=bnpo]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:28<00:01,  3.73it/s, loss=0.212, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:30<00:00,  3.70it/s, loss=0.212, v_num=bnpo]\n",
      "Validating:  40%|████      | 2/5 [00:05<00:07,  2.54s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [04:35<00:00,  3.65it/s, loss=0.212, v_num=bnpo]\n",
      "Validating:  80%|████████  | 4/5 [00:09<00:02,  2.19s/it]\u001b[A\n",
      "Epoch 1: 100%|██████████| 1005/1005 [04:39<00:00,  3.59it/s, loss=0.212, v_num=bnpo]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:28<00:01,  3.72it/s, loss=0.214, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:32<00:00,  3.68it/s, loss=0.214, v_num=bnpo]\n",
      "Validating:  40%|████      | 2/5 [00:05<00:08,  2.94s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [04:37<00:00,  3.62it/s, loss=0.214, v_num=bnpo]\n",
      "Validating:  80%|████████  | 4/5 [00:11<00:02,  2.75s/it]\u001b[A\n",
      "Epoch 2: 100%|██████████| 1005/1005 [04:43<00:00,  3.55it/s, loss=0.214, v_num=bnpo]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:29<00:01,  3.71it/s, loss=0.211, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:32<00:00,  3.67it/s, loss=0.211, v_num=bnpo]\n",
      "Validating:  40%|████      | 2/5 [00:04<00:06,  2.15s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [04:36<00:00,  3.64it/s, loss=0.211, v_num=bnpo]\n",
      "Validating:  80%|████████  | 4/5 [00:08<00:01,  1.97s/it]\u001b[A\n",
      "Epoch 3: 100%|██████████| 1005/1005 [04:40<00:00,  3.58it/s, loss=0.211, v_num=bnpo]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:28<00:01,  3.73it/s, loss=0.209, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:31<00:00,  3.69it/s, loss=0.209, v_num=bnpo]\n",
      "Validating:  40%|████      | 2/5 [00:04<00:06,  2.25s/it]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [04:34<00:00,  3.65it/s, loss=0.209, v_num=bnpo]\n",
      "Validating:  80%|████████  | 4/5 [00:08<00:01,  1.99s/it]\u001b[A\n",
      "Epoch 4: 100%|██████████| 1005/1005 [04:39<00:00,  3.59it/s, loss=0.209, v_num=bnpo]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:25<00:01,  3.77it/s, loss=0.197, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:27<00:00,  3.74it/s, loss=0.197, v_num=bnpo]\n",
      "Validating:  40%|████      | 2/5 [00:03<00:04,  1.47s/it]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [04:29<00:00,  3.72it/s, loss=0.197, v_num=bnpo]\n",
      "Validating:  80%|████████  | 4/5 [00:05<00:01,  1.28s/it]\u001b[A\n",
      "Epoch 5: 100%|██████████| 1005/1005 [04:32<00:00,  3.69it/s, loss=0.197, v_num=bnpo]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:25<00:01,  3.77it/s, loss=0.196, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:27<00:00,  3.75it/s, loss=0.196, v_num=bnpo]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.17s/it]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [04:28<00:00,  3.73it/s, loss=0.196, v_num=bnpo]\n",
      "Validating:  80%|████████  | 4/5 [00:04<00:00,  1.04it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 1005/1005 [04:30<00:00,  3.71it/s, loss=0.196, v_num=bnpo]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:25<00:01,  3.76it/s, loss=0.196, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:27<00:00,  3.75it/s, loss=0.196, v_num=bnpo]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.05s/it]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [04:28<00:00,  3.73it/s, loss=0.196, v_num=bnpo]\n",
      "Validating:  80%|████████  | 4/5 [00:03<00:00,  1.18it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 1005/1005 [04:30<00:00,  3.71it/s, loss=0.196, v_num=bnpo]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:25<00:01,  3.77it/s, loss=0.197, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:26<00:00,  3.76it/s, loss=0.197, v_num=bnpo]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:02,  1.00it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [04:28<00:00,  3.74it/s, loss=0.197, v_num=bnpo]\n",
      "Validating:  80%|████████  | 4/5 [00:03<00:00,  1.26it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 1005/1005 [04:29<00:00,  3.72it/s, loss=0.197, v_num=bnpo]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:25<00:01,  3.76it/s, loss=0.195, v_num=bnpo]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:27<00:00,  3.75it/s, loss=0.195, v_num=bnpo]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:02,  1.12it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [04:28<00:00,  3.74it/s, loss=0.195, v_num=bnpo]\n",
      "Validating:  80%|████████  | 4/5 [00:03<00:00,  1.44it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 1005/1005 [04:29<00:00,  3.72it/s, loss=0.195, v_num=bnpo]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [04:30<00:00,  3.72it/s, loss=0.195, v_num=bnpo]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 122049... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▁█████████</td></tr><tr><td>cut_pur</td><td>▁▃▂▃▃▅▆▇▇█</td></tr><tr><td>dist@0.98</td><td>▁▇▆▆▇▇▇███</td></tr><tr><td>eff</td><td>█▅▅▄▄▃▂▂▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▁▃▂▃▃▅▆▇▇█</td></tr><tr><td>train_loss</td><td>▇▅▆▆▆▆▄▄▄▅▆▇▄▃█▇▄▃▃▃▃▃▂▂▃▂▁▂▃▁▁▂▂▂▂▂▁▁▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97775</td></tr><tr><td>cut_pur</td><td>0.00055</td></tr><tr><td>dist@0.98</td><td>0.41512</td></tr><tr><td>eff</td><td>0.86346</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00306</td></tr><tr><td>train_loss</td><td>0.19862</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">still-sweep-11</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/8il3bnpo\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/8il3bnpo</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_081348-8il3bnpo/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: af8y5vvg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 1\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/af8y5vvg\" target=\"_blank\">smooth-sweep-12</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:28<00:01,  3.73it/s, loss=0.227, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:31<00:00,  3.69it/s, loss=0.227, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:04<00:07,  2.38s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [04:35<00:00,  3.64it/s, loss=0.227, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:09<00:02,  2.12s/it]\u001b[A\n",
      "Epoch 0: 100%|██████████| 1005/1005 [04:40<00:00,  3.59it/s, loss=0.227, v_num=5vvg]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:27<00:01,  3.74it/s, loss=0.218, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:31<00:00,  3.69it/s, loss=0.218, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:06<00:09,  3.04s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [04:36<00:00,  3.63it/s, loss=0.218, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:11<00:02,  2.67s/it]\u001b[A\n",
      "Epoch 1: 100%|██████████| 1005/1005 [04:42<00:00,  3.56it/s, loss=0.218, v_num=5vvg]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:28<00:01,  3.72it/s, loss=0.214, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:31<00:00,  3.69it/s, loss=0.214, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:04<00:07,  2.36s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [04:35<00:00,  3.64it/s, loss=0.214, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:08<00:02,  2.07s/it]\u001b[A\n",
      "Epoch 2: 100%|██████████| 1005/1005 [04:40<00:00,  3.59it/s, loss=0.214, v_num=5vvg]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:28<00:01,  3.72it/s, loss=0.214, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:31<00:00,  3.69it/s, loss=0.214, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:05<00:07,  2.50s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [04:36<00:00,  3.64it/s, loss=0.214, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:09<00:02,  2.15s/it]\u001b[A\n",
      "Epoch 3: 100%|██████████| 1005/1005 [04:40<00:00,  3.58it/s, loss=0.214, v_num=5vvg]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:28<00:01,  3.73it/s, loss=0.217, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:31<00:00,  3.69it/s, loss=0.217, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:05<00:07,  2.45s/it]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [04:35<00:00,  3.64it/s, loss=0.217, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:09<00:02,  2.14s/it]\u001b[A\n",
      "Epoch 4: 100%|██████████| 1005/1005 [04:39<00:00,  3.59it/s, loss=0.217, v_num=5vvg]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:27<00:01,  3.74it/s, loss=0.202, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:29<00:00,  3.72it/s, loss=0.202, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:03<00:04,  1.48s/it]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [04:31<00:00,  3.69it/s, loss=0.202, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:05<00:01,  1.21s/it]\u001b[A\n",
      "Epoch 5: 100%|██████████| 1005/1005 [04:34<00:00,  3.66it/s, loss=0.202, v_num=5vvg]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:24<00:01,  3.78it/s, loss=0.201, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:26<00:00,  3.76it/s, loss=0.201, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.16s/it]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [04:28<00:00,  3.74it/s, loss=0.201, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:04<00:00,  1.10it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 1005/1005 [04:29<00:00,  3.72it/s, loss=0.201, v_num=5vvg]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:23<00:01,  3.79it/s, loss=0.199, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:25<00:00,  3.78it/s, loss=0.199, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:02,  1.01it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [04:26<00:00,  3.77it/s, loss=0.199, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:03<00:00,  1.30it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 1005/1005 [04:28<00:00,  3.75it/s, loss=0.199, v_num=5vvg]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:25<00:01,  3.77it/s, loss=0.198, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:26<00:00,  3.76it/s, loss=0.198, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:02,  1.15it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [04:28<00:00,  3.75it/s, loss=0.198, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:03<00:00,  1.48it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 1005/1005 [04:29<00:00,  3.73it/s, loss=0.198, v_num=5vvg]\n",
      "Epoch 9: 100%|█████████▉| 1000/1005 [04:26<00:01,  3.76it/s, loss=0.201, v_num=5vvg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1002/1005 [04:27<00:00,  3.74it/s, loss=0.201, v_num=5vvg]\n",
      "Validating:  40%|████      | 2/5 [00:01<00:02,  1.14it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1004/1005 [04:28<00:00,  3.73it/s, loss=0.201, v_num=5vvg]\n",
      "Validating:  80%|████████  | 4/5 [00:03<00:00,  1.47it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 1005/1005 [04:30<00:00,  3.72it/s, loss=0.201, v_num=5vvg]\n",
      "Epoch 9: 100%|██████████| 1005/1005 [04:30<00:00,  3.71it/s, loss=0.201, v_num=5vvg]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 157042... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>████▃▃▃▃▃▁</td></tr><tr><td>cut_eff</td><td>▇▁▆▅▂▇█▇██</td></tr><tr><td>cut_pur</td><td>▂▁▂▂▂▄▆▇██</td></tr><tr><td>dist@0.98</td><td>▂▁▇▆▄▇███▇</td></tr><tr><td>eff</td><td>▇█▇▆▅▃▃▂▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>pur</td><td>▂▁▂▂▂▄▆▇██</td></tr><tr><td>train_loss</td><td>▅▄▄▄▄▃▄▄▅▄▄█▅▃▃▂▃▃▃▃▃▂▂▃▃▃▂▃▂▃▂▂▂▂▂▁▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>current_lr</td><td>0.0</td></tr><tr><td>cut_eff</td><td>0.97767</td></tr><tr><td>cut_pur</td><td>0.00055</td></tr><tr><td>dist@0.98</td><td>0.38314</td></tr><tr><td>eff</td><td>0.8621</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>pur</td><td>0.00307</td></tr><tr><td>train_loss</td><td>0.19628</td></tr><tr><td>trainer/global_step</td><td>9999</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">smooth-sweep-12</strong>: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/af8y5vvg\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/af8y5vvg</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220310_090031-af8y5vvg/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dinqaylg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_ratio: 1.5\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/dinqaylg\" target=\"_blank\">driven-sweep-13</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/9ycd43iv</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_ratio' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'margin' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█████████▉| 1000/1005 [04:32<00:01,  3.67it/s, loss=0.233, v_num=aylg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1002/1005 [04:39<00:00,  3.59it/s, loss=0.233, v_num=aylg]\n",
      "Validating:  40%|████      | 2/5 [00:12<00:19,  6.39s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1004/1005 [04:51<00:00,  3.44it/s, loss=0.233, v_num=aylg]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [04:57<00:00,  3.38it/s, loss=0.233, v_num=aylg]\n",
      "Epoch 0: 100%|██████████| 1005/1005 [05:03<00:00,  3.31it/s, loss=0.233, v_num=aylg]\n",
      "Epoch 1: 100%|█████████▉| 1000/1005 [04:33<00:01,  3.66it/s, loss=0.212, v_num=aylg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1002/1005 [04:38<00:00,  3.60it/s, loss=0.212, v_num=aylg]\n",
      "Validating:  40%|████      | 2/5 [00:08<00:12,  4.11s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1004/1005 [04:45<00:00,  3.51it/s, loss=0.212, v_num=aylg]\n",
      "Validating:  80%|████████  | 4/5 [00:15<00:03,  3.70s/it]\u001b[A\n",
      "Epoch 1: 100%|██████████| 1005/1005 [04:53<00:00,  3.43it/s, loss=0.212, v_num=aylg]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:30<00:01,  3.70it/s, loss=0.21, v_num=aylg] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [04:34<00:00,  3.66it/s, loss=0.21, v_num=aylg]\n",
      "Validating:  40%|████      | 2/5 [00:07<00:10,  3.57s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [04:40<00:00,  3.58it/s, loss=0.21, v_num=aylg]\n",
      "Validating:  80%|████████  | 4/5 [00:13<00:03,  3.23s/it]\u001b[A\n",
      "Epoch 2: 100%|██████████| 1005/1005 [04:46<00:00,  3.50it/s, loss=0.21, v_num=aylg]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:28<00:01,  3.72it/s, loss=0.206, v_num=aylg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:31<00:00,  3.69it/s, loss=0.206, v_num=aylg]\n",
      "Validating:  40%|████      | 2/5 [00:05<00:08,  2.86s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [04:37<00:00,  3.62it/s, loss=0.206, v_num=aylg]\n",
      "Validating:  80%|████████  | 4/5 [00:10<00:02,  2.64s/it]\u001b[A\n",
      "Epoch 3: 100%|██████████| 1005/1005 [04:42<00:00,  3.56it/s, loss=0.206, v_num=aylg]\n",
      "Epoch 6: 100%|█████████▉| 1000/1005 [04:25<00:01,  3.76it/s, loss=0.197, v_num=aylg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1002/1005 [04:27<00:00,  3.74it/s, loss=0.197, v_num=aylg]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.25s/it]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1004/1005 [04:29<00:00,  3.72it/s, loss=0.197, v_num=aylg]\n",
      "Validating:  80%|████████  | 4/5 [00:04<00:01,  1.01s/it]\u001b[A\n",
      "Epoch 6: 100%|██████████| 1005/1005 [04:31<00:00,  3.70it/s, loss=0.197, v_num=aylg]\n",
      "Epoch 7: 100%|█████████▉| 1000/1005 [04:27<00:01,  3.74it/s, loss=0.194, v_num=aylg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1002/1005 [04:28<00:00,  3.73it/s, loss=0.194, v_num=aylg]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.14s/it]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1004/1005 [04:30<00:00,  3.71it/s, loss=0.194, v_num=aylg]\n",
      "Validating:  80%|████████  | 4/5 [00:04<00:00,  1.10it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 1005/1005 [04:32<00:00,  3.69it/s, loss=0.194, v_num=aylg]\n",
      "Epoch 8: 100%|█████████▉| 1000/1005 [04:26<00:01,  3.75it/s, loss=0.195, v_num=aylg]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1002/1005 [04:27<00:00,  3.74it/s, loss=0.195, v_num=aylg]\n",
      "Validating:  40%|████      | 2/5 [00:02<00:03,  1.03s/it]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1004/1005 [04:29<00:00,  3.73it/s, loss=0.195, v_num=aylg]\n",
      "Validating:  80%|████████  | 4/5 [00:03<00:00,  1.27it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 1005/1005 [04:30<00:00,  3.71it/s, loss=0.195, v_num=aylg]\n",
      "Epoch 2: 100%|█████████▉| 1000/1005 [04:53<00:01,  3.41it/s, loss=0.215, v_num=xd2u]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1002/1005 [05:05<00:00,  3.28it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 2: 100%|█████████▉| 1003/1005 [05:15<00:00,  3.18it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 2: 100%|█████████▉| 1004/1005 [05:26<00:00,  3.08it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:36<00:00,  2.99it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 2: 100%|██████████| 1005/1005 [05:47<00:00,  2.89it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 3: 100%|█████████▉| 1000/1005 [04:39<00:01,  3.58it/s, loss=0.215, v_num=xd2u]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1002/1005 [04:50<00:00,  3.45it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 3: 100%|█████████▉| 1003/1005 [04:59<00:00,  3.35it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 3: 100%|█████████▉| 1004/1005 [05:10<00:00,  3.24it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:19<00:00,  3.14it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 3: 100%|██████████| 1005/1005 [05:30<00:00,  3.04it/s, loss=0.215, v_num=xd2u]\n",
      "Epoch 4: 100%|█████████▉| 1000/1005 [04:38<00:01,  3.59it/s, loss=0.214, v_num=xd2u]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1002/1005 [04:49<00:00,  3.47it/s, loss=0.214, v_num=xd2u]\n",
      "Epoch 4: 100%|█████████▉| 1003/1005 [04:58<00:00,  3.36it/s, loss=0.214, v_num=xd2u]\n",
      "Epoch 4: 100%|█████████▉| 1004/1005 [05:08<00:00,  3.26it/s, loss=0.214, v_num=xd2u]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:16<00:00,  3.18it/s, loss=0.214, v_num=xd2u]\n",
      "Epoch 4: 100%|██████████| 1005/1005 [05:26<00:00,  3.07it/s, loss=0.214, v_num=xd2u]\n",
      "Epoch 5: 100%|█████████▉| 1000/1005 [04:36<00:01,  3.61it/s, loss=0.213, v_num=xd2u]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1002/1005 [04:47<00:00,  3.49it/s, loss=0.213, v_num=xd2u]\n",
      "Epoch 5: 100%|█████████▉| 1003/1005 [04:56<00:00,  3.39it/s, loss=0.213, v_num=xd2u]\n",
      "Epoch 5: 100%|█████████▉| 1004/1005 [05:05<00:00,  3.29it/s, loss=0.213, v_num=xd2u]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:13<00:00,  3.20it/s, loss=0.213, v_num=xd2u]\n",
      "Epoch 5: 100%|██████████| 1005/1005 [05:24<00:00,  3.10it/s, loss=0.213, v_num=xd2u]\n",
      "Epoch 6:  24%|██▍       | 242/1005 [01:08<03:36,  3.53it/s, loss=0.213, v_num=xd2u] "
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_configuration, project = \"ITk_barrel_dual_embedding\")\n",
    "\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id, function=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Construct PyLightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dual_embedding_default.yaml\") as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaDualEmbedding(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metric Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='pur',\n",
    "    mode=\"max\",\n",
    "    save_top_k=2,\n",
    "    save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "kaiming_init(model)\n",
    "logger = WandbLogger(project=\"ITk_dual_embedding\")\n",
    "trainer = Trainer(gpus=1, max_epochs=hparams[\"max_epochs\"], logger=logger, num_sanity_val_steps=2, callbacks=[checkpoint_callback], log_every_n_steps = 50, default_root_dir=\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_embedding/\")\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "\n",
    "with open(\"dual_embedding_default.yaml\") as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "hparams[\"use_dual_encoder\"] = True\n",
    "\n",
    "model = VanillaDualEmbedding(hparams)\n",
    "\n",
    "checkpoint = torch.load(\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_embedding/ITk_dual_embedding/3ijb4qnw/checkpoints/last.ckpt\")\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "names = [i for i in state_dict]\n",
    "for i in names:\n",
    "    state = state_dict[i]\n",
    "    i = i.replace(\"input_layer1\", \"input_layer2\")\n",
    "    i = i.replace(\"layers1\", \"layers2\")\n",
    "    i = i.replace(\"output_layer1\", \"output_layer2\")\n",
    "    state_dict[i] = state\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='pur',\n",
    "    mode=\"max\",\n",
    "    save_top_k=2,\n",
    "    save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = WandbLogger(project=\"ITk_dual_embedding\")\n",
    "trainer = Trainer(gpus=1, max_epochs=hparams[\"max_epochs\"], logger=logger, num_sanity_val_steps=2, callbacks=[checkpoint_callback], log_every_n_steps = 50, default_root_dir=\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_embedding/\")\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
