{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " architectures\n"
     ]
    }
   ],
   "source": [
    "run_name = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# External imports\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "import frnn\n",
    "import wandb\n",
    "import math\n",
    "sys.path.append('../..')\n",
    "\n",
    "from LightningModules.DualEmbedding.Models.vanilla_dual_embedding import VanillaDualEmbedding\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if name.endswith(\".bias\"):\n",
    "            param.data.fill_(0)\n",
    "        elif name.startswith(\"layers.0\"):  # The first layer does not have ReLU applied on its input\n",
    "            param.data.normal_(0, 1 / math.sqrt(param.shape[1]))\n",
    "        else:\n",
    "            param.data.normal_(0, math.sqrt(2) / math.sqrt(param.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dual_embedding_sweep.yaml\") as f:\n",
    "        sweep_hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "with open(\"dual_embedding_default.yaml\") as f:\n",
    "        default_hparams = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_configuration = {\n",
    "    \"name\": run_name,\n",
    "    \"project\": \"ITk_barrell_dual_embedding\",\n",
    "    \"metric\": {\"name\": \"pur\", \"goal\": \"maximize\"},\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": sweep_hparams\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(model):\n",
    "    checkpoint = torch.load(\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_embedding/ITk_dual_embedding/3ijb4qnw/checkpoints/last.ckpt\")\n",
    "    state_dict = checkpoint[\"state_dict\"]\n",
    "    names = [i for i in state_dict]\n",
    "    for i in names:\n",
    "        state = state_dict[i]\n",
    "        i = i.replace(\"input_layer1\", \"input_layer2\")\n",
    "        i = i.replace(\"layers1\", \"layers2\")\n",
    "        i = i.replace(\"output_layer1\", \"output_layer2\")\n",
    "        state_dict[i] = state\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    del state_dict\n",
    "    del checkpoint\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    wandb.init()\n",
    "    model = VanillaDualEmbedding({**default_hparams, **wandb.config})\n",
    "    \n",
    "    if model.hparams[\"use_dual_encoder\"]:\n",
    "        model = load_dict(model)\n",
    "    else:\n",
    "        kaiming_init(model)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='pur',\n",
    "        mode=\"max\",\n",
    "        save_top_k=2,\n",
    "        save_last=True)\n",
    "\n",
    "    logger = WandbLogger()\n",
    "    trainer = Trainer(gpus=1, max_epochs=default_hparams[\"max_epochs\"], log_every_n_steps = 50, logger=logger, callbacks=[checkpoint_callback], default_root_dir=\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_dual_embedding/\")\n",
    "    trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 6y0m08n8\n",
      "Sweep URL: https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/6y0m08n8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hv4fwgw9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_bidir_truth: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_dual_encoder: True\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mexatrkx\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/runs/hv4fwgw9\" target=\"_blank\">worthy-sweep-1</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "Sweep page: <a href=\"https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/6y0m08n8\" target=\"_blank\">https://wandb.ai/exatrkx/ITk_barrel_dual_embedding/sweeps/6y0m08n8</a><br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/loggers/wandb.py:341: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_bidir_truth' was locked by 'sweep' (ignored update).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'use_dual_encoder' was locked by 'sweep' (ignored update).\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 1000/1010 [03:56<00:02,  4.22it/s, loss=0.121, v_num=wgw9]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1002/1010 [03:58<00:01,  4.20it/s, loss=0.121, v_num=wgw9]\n",
      "Validating:  20%|██        | 2/10 [00:02<00:08,  1.11s/it]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1004/1010 [04:00<00:01,  4.18it/s, loss=0.121, v_num=wgw9]\n",
      "Validating:  40%|████      | 4/10 [00:04<00:05,  1.03it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1006/1010 [04:02<00:00,  4.16it/s, loss=0.121, v_num=wgw9]\n",
      "Validating:  60%|██████    | 6/10 [00:05<00:03,  1.23it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1008/1010 [04:03<00:00,  4.14it/s, loss=0.121, v_num=wgw9]\n",
      "Validating:  80%|████████  | 8/10 [00:06<00:01,  1.44it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 1010/1010 [04:04<00:00,  4.13it/s, loss=0.121, v_num=wgw9]\n",
      "Epoch 0: 100%|██████████| 1010/1010 [04:05<00:00,  4.11it/s, loss=0.121, v_num=wgw9]\n",
      "Epoch 1:  99%|█████████▉| 1000/1010 [04:03<00:02,  4.10it/s, loss=0.119, v_num=wgw9]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 1002/1010 [04:05<00:01,  4.09it/s, loss=0.119, v_num=wgw9]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:06,  1.33it/s]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 1004/1010 [04:06<00:01,  4.08it/s, loss=0.119, v_num=wgw9]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.51it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1006/1010 [04:07<00:00,  4.07it/s, loss=0.119, v_num=wgw9]\n",
      "Validating:  60%|██████    | 6/10 [00:03<00:02,  1.87it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1008/1010 [04:08<00:00,  4.06it/s, loss=0.119, v_num=wgw9]\n",
      "Validating:  80%|████████  | 8/10 [00:04<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 1010/1010 [04:08<00:00,  4.06it/s, loss=0.119, v_num=wgw9]\n",
      "Epoch 1: 100%|██████████| 1010/1010 [04:09<00:00,  4.05it/s, loss=0.119, v_num=wgw9]\n",
      "Epoch 2:  99%|█████████▉| 1000/1010 [04:04<00:02,  4.09it/s, loss=0.12, v_num=wgw9] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 1002/1010 [04:05<00:01,  4.08it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:05,  1.58it/s]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 1004/1010 [04:06<00:01,  4.07it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.66it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1006/1010 [04:07<00:00,  4.06it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  60%|██████    | 6/10 [00:03<00:01,  2.04it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1008/1010 [04:08<00:00,  4.06it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  80%|████████  | 8/10 [00:04<00:00,  2.44it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 1010/1010 [04:09<00:00,  4.06it/s, loss=0.12, v_num=wgw9]\n",
      "Epoch 2: 100%|██████████| 1010/1010 [04:09<00:00,  4.05it/s, loss=0.12, v_num=wgw9]\n",
      "Epoch 3:  99%|█████████▉| 1000/1010 [04:05<00:02,  4.08it/s, loss=0.12, v_num=wgw9] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 1002/1010 [04:06<00:01,  4.07it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:04,  1.76it/s]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 1004/1010 [04:06<00:01,  4.07it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.99it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1006/1010 [04:07<00:00,  4.06it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  60%|██████    | 6/10 [00:02<00:01,  2.42it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1008/1010 [04:08<00:00,  4.06it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  80%|████████  | 8/10 [00:03<00:00,  2.93it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 1010/1010 [04:09<00:00,  4.06it/s, loss=0.12, v_num=wgw9]\n",
      "Epoch 3: 100%|██████████| 1010/1010 [04:09<00:00,  4.05it/s, loss=0.12, v_num=wgw9]\n",
      "Epoch 4:  99%|█████████▉| 1000/1010 [04:06<00:02,  4.06it/s, loss=0.12, v_num=wgw9]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  99%|█████████▉| 1002/1010 [04:07<00:01,  4.05it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:05,  1.35it/s]\u001b[A\n",
      "Epoch 4:  99%|█████████▉| 1004/1010 [04:08<00:01,  4.04it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.53it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1006/1010 [04:09<00:00,  4.03it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  60%|██████    | 6/10 [00:03<00:02,  1.89it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1008/1010 [04:10<00:00,  4.02it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  80%|████████  | 8/10 [00:04<00:00,  2.28it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 1010/1010 [04:11<00:00,  4.02it/s, loss=0.12, v_num=wgw9]\n",
      "Epoch 4: 100%|██████████| 1010/1010 [04:11<00:00,  4.01it/s, loss=0.12, v_num=wgw9]\n",
      "Epoch 5:  99%|█████████▉| 1000/1010 [04:05<00:02,  4.07it/s, loss=0.118, v_num=wgw9]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  99%|█████████▉| 1002/1010 [04:06<00:01,  4.06it/s, loss=0.118, v_num=wgw9]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:04,  1.73it/s]\u001b[A\n",
      "Epoch 5:  99%|█████████▉| 1004/1010 [04:07<00:01,  4.06it/s, loss=0.118, v_num=wgw9]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:02,  2.09it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1006/1010 [04:08<00:00,  4.05it/s, loss=0.118, v_num=wgw9]\n",
      "Validating:  60%|██████    | 6/10 [00:02<00:01,  2.50it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1008/1010 [04:08<00:00,  4.05it/s, loss=0.118, v_num=wgw9]\n",
      "Validating:  80%|████████  | 8/10 [00:03<00:00,  3.07it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 1010/1010 [04:09<00:00,  4.05it/s, loss=0.118, v_num=wgw9]\n",
      "Epoch 5: 100%|██████████| 1010/1010 [04:09<00:00,  4.04it/s, loss=0.118, v_num=wgw9]\n",
      "Epoch 6:  99%|█████████▉| 1000/1010 [04:07<00:02,  4.04it/s, loss=0.119, v_num=wgw9]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  99%|█████████▉| 1002/1010 [04:08<00:01,  4.04it/s, loss=0.119, v_num=wgw9]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:04,  1.98it/s]\u001b[A\n",
      "Epoch 6:  99%|█████████▉| 1004/1010 [04:08<00:01,  4.03it/s, loss=0.119, v_num=wgw9]\n",
      "Validating:  40%|████      | 4/10 [00:01<00:02,  2.15it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1006/1010 [04:09<00:00,  4.03it/s, loss=0.119, v_num=wgw9]\n",
      "Validating:  60%|██████    | 6/10 [00:02<00:01,  2.48it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1008/1010 [04:10<00:00,  4.03it/s, loss=0.119, v_num=wgw9]\n",
      "Validating:  80%|████████  | 8/10 [00:03<00:00,  2.96it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 1010/1010 [04:11<00:00,  4.02it/s, loss=0.119, v_num=wgw9]\n",
      "Epoch 6: 100%|██████████| 1010/1010 [04:11<00:00,  4.02it/s, loss=0.119, v_num=wgw9]\n",
      "Epoch 7:  99%|█████████▉| 1000/1010 [04:09<00:02,  4.01it/s, loss=0.12, v_num=wgw9] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  99%|█████████▉| 1002/1010 [04:10<00:01,  4.00it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:04,  1.99it/s]\u001b[A\n",
      "Epoch 7:  99%|█████████▉| 1004/1010 [04:11<00:01,  4.00it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  40%|████      | 4/10 [00:01<00:02,  2.12it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1006/1010 [04:11<00:01,  3.99it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  60%|██████    | 6/10 [00:02<00:01,  2.47it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1008/1010 [04:12<00:00,  3.99it/s, loss=0.12, v_num=wgw9]\n",
      "Validating:  80%|████████  | 8/10 [00:03<00:00,  2.99it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 1010/1010 [04:13<00:00,  3.99it/s, loss=0.12, v_num=wgw9]\n",
      "Epoch 7: 100%|██████████| 1010/1010 [04:13<00:00,  3.98it/s, loss=0.12, v_num=wgw9]\n",
      "Epoch 8:  76%|███████▌  | 765/1010 [03:09<01:00,  4.03it/s, loss=0.119, v_num=wgw9]"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_configuration, project = \"ITk_barrel_dual_embedding\")\n",
    "\n",
    "# run the sweep\n",
    "wandb.agent(sweep_id, function=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Construct PyLightning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dual_embedding_default.yaml\") as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VanillaDualEmbedding(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Metric Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='pur',\n",
    "    mode=\"max\",\n",
    "    save_top_k=2,\n",
    "    save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mexatrkx\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_dual_embedding/runs/1pcbq6d8\" target=\"_blank\">swept-monkey-30</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:02<00:02,  2.63s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 1000/1010 [04:17<00:02,  3.88it/s, loss=0.0983, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1002/1010 [04:20<00:02,  3.84it/s, loss=0.0983, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:05<00:20,  2.52s/it]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1004/1010 [04:25<00:01,  3.78it/s, loss=0.0983, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:10<00:14,  2.49s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1006/1010 [04:30<00:01,  3.72it/s, loss=0.0983, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:14<00:09,  2.32s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1008/1010 [04:34<00:00,  3.67it/s, loss=0.0983, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:18<00:04,  2.08s/it]\u001b[A\n",
      "Epoch 0: 100%|██████████| 1010/1010 [04:38<00:00,  3.63it/s, loss=0.0983, v_num=q6d8]\n",
      "Epoch 0: 100%|██████████| 1010/1010 [04:40<00:00,  3.60it/s, loss=0.0983, v_num=q6d8]\n",
      "Epoch 1:  99%|█████████▉| 1000/1010 [03:50<00:02,  4.34it/s, loss=0.102, v_num=q6d8] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 1002/1010 [03:52<00:01,  4.30it/s, loss=0.102, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:03<00:14,  1.86s/it]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 1004/1010 [03:56<00:01,  4.25it/s, loss=0.102, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:07<00:10,  1.80s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1006/1010 [03:59<00:00,  4.20it/s, loss=0.102, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:11<00:07,  1.80s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1008/1010 [04:02<00:00,  4.15it/s, loss=0.102, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:13<00:03,  1.55s/it]\u001b[A\n",
      "Epoch 1: 100%|██████████| 1010/1010 [04:05<00:00,  4.11it/s, loss=0.102, v_num=q6d8]\n",
      "Epoch 1: 100%|██████████| 1010/1010 [04:07<00:00,  4.08it/s, loss=0.102, v_num=q6d8]\n",
      "Epoch 2:  99%|█████████▉| 1000/1010 [03:52<00:02,  4.29it/s, loss=0.102, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 1002/1010 [03:54<00:01,  4.27it/s, loss=0.102, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:03<00:11,  1.48s/it]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 1004/1010 [03:57<00:01,  4.23it/s, loss=0.102, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:05<00:08,  1.38s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1006/1010 [03:59<00:00,  4.19it/s, loss=0.102, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:08<00:04,  1.23s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1008/1010 [04:01<00:00,  4.17it/s, loss=0.102, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:09<00:02,  1.08s/it]\u001b[A\n",
      "Epoch 2: 100%|██████████| 1010/1010 [04:04<00:00,  4.14it/s, loss=0.102, v_num=q6d8]\n",
      "Epoch 2: 100%|██████████| 1010/1010 [04:05<00:00,  4.12it/s, loss=0.102, v_num=q6d8]\n",
      "Epoch 3:  99%|█████████▉| 1000/1010 [03:54<00:02,  4.26it/s, loss=0.105, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 1002/1010 [03:56<00:01,  4.23it/s, loss=0.105, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:02<00:11,  1.40s/it]\u001b[A\n",
      "Epoch 3:  99%|█████████▉| 1004/1010 [03:59<00:01,  4.20it/s, loss=0.105, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:05<00:07,  1.29s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1006/1010 [04:01<00:00,  4.16it/s, loss=0.105, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:07<00:04,  1.15s/it]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 1008/1010 [04:03<00:00,  4.14it/s, loss=0.105, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:09<00:01,  1.01it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 1010/1010 [04:05<00:00,  4.12it/s, loss=0.105, v_num=q6d8]\n",
      "Epoch 3: 100%|██████████| 1010/1010 [04:06<00:00,  4.10it/s, loss=0.105, v_num=q6d8]\n",
      "Epoch 4:  99%|█████████▉| 1000/1010 [03:56<00:02,  4.23it/s, loss=0.106, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 4:  99%|█████████▉| 1002/1010 [03:58<00:01,  4.21it/s, loss=0.106, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:02<00:09,  1.23s/it]\u001b[A\n",
      "Epoch 4:  99%|█████████▉| 1004/1010 [04:00<00:01,  4.18it/s, loss=0.106, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:04<00:06,  1.16s/it]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1006/1010 [04:02<00:00,  4.15it/s, loss=0.106, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:06<00:03,  1.00it/s]\u001b[A\n",
      "Epoch 4: 100%|█████████▉| 1008/1010 [04:04<00:00,  4.13it/s, loss=0.106, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:08<00:01,  1.12it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 1010/1010 [04:05<00:00,  4.11it/s, loss=0.106, v_num=q6d8]\n",
      "Epoch 4: 100%|██████████| 1010/1010 [04:07<00:00,  4.09it/s, loss=0.106, v_num=q6d8]\n",
      "Epoch 5:  99%|█████████▉| 1000/1010 [03:55<00:02,  4.25it/s, loss=0.108, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 5:  99%|█████████▉| 1002/1010 [03:56<00:01,  4.23it/s, loss=0.108, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:02<00:09,  1.14s/it]\u001b[A\n",
      "Epoch 5:  99%|█████████▉| 1004/1010 [03:58<00:01,  4.21it/s, loss=0.108, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:04<00:06,  1.03s/it]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1006/1010 [04:00<00:00,  4.18it/s, loss=0.108, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:06<00:03,  1.11it/s]\u001b[A\n",
      "Epoch 5: 100%|█████████▉| 1008/1010 [04:02<00:00,  4.16it/s, loss=0.108, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:07<00:01,  1.27it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 1010/1010 [04:03<00:00,  4.15it/s, loss=0.108, v_num=q6d8]\n",
      "Epoch 5: 100%|██████████| 1010/1010 [04:04<00:00,  4.13it/s, loss=0.108, v_num=q6d8]\n",
      "Epoch 6:  99%|█████████▉| 1000/1010 [03:53<00:02,  4.29it/s, loss=0.11, v_num=q6d8] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 6:  99%|█████████▉| 1002/1010 [03:54<00:01,  4.27it/s, loss=0.11, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:02<00:08,  1.08s/it]\u001b[A\n",
      "Epoch 6:  99%|█████████▉| 1004/1010 [03:56<00:01,  4.25it/s, loss=0.11, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:04<00:05,  1.06it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1006/1010 [03:57<00:00,  4.23it/s, loss=0.11, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:05<00:03,  1.26it/s]\u001b[A\n",
      "Epoch 6: 100%|█████████▉| 1008/1010 [03:59<00:00,  4.21it/s, loss=0.11, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:06<00:01,  1.42it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 1010/1010 [04:00<00:00,  4.20it/s, loss=0.11, v_num=q6d8]\n",
      "Epoch 6: 100%|██████████| 1010/1010 [04:01<00:00,  4.18it/s, loss=0.11, v_num=q6d8]\n",
      "Epoch 7:  99%|█████████▉| 1000/1010 [03:55<00:02,  4.24it/s, loss=0.11, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 7:  99%|█████████▉| 1002/1010 [03:56<00:01,  4.23it/s, loss=0.11, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:02<00:07,  1.03it/s]\u001b[A\n",
      "Epoch 7:  99%|█████████▉| 1004/1010 [03:58<00:01,  4.21it/s, loss=0.11, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:03<00:05,  1.12it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1006/1010 [04:00<00:00,  4.19it/s, loss=0.11, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:05<00:03,  1.29it/s]\u001b[A\n",
      "Epoch 7: 100%|█████████▉| 1008/1010 [04:01<00:00,  4.18it/s, loss=0.11, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:06<00:01,  1.44it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 1010/1010 [04:02<00:00,  4.16it/s, loss=0.11, v_num=q6d8]\n",
      "Epoch 7: 100%|██████████| 1010/1010 [04:03<00:00,  4.15it/s, loss=0.11, v_num=q6d8]\n",
      "Epoch 8:  99%|█████████▉| 1000/1010 [03:54<00:02,  4.27it/s, loss=0.111, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 8:  99%|█████████▉| 1002/1010 [03:55<00:01,  4.25it/s, loss=0.111, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:06,  1.15it/s]\u001b[A\n",
      "Epoch 8:  99%|█████████▉| 1004/1010 [03:57<00:01,  4.23it/s, loss=0.111, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:03<00:04,  1.26it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1006/1010 [03:58<00:00,  4.22it/s, loss=0.111, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:04<00:02,  1.48it/s]\u001b[A\n",
      "Epoch 8: 100%|█████████▉| 1008/1010 [03:59<00:00,  4.21it/s, loss=0.111, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:05<00:01,  1.71it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 1010/1010 [04:00<00:00,  4.20it/s, loss=0.111, v_num=q6d8]\n",
      "Epoch 8: 100%|██████████| 1010/1010 [04:01<00:00,  4.18it/s, loss=0.111, v_num=q6d8]\n",
      "Epoch 9:  99%|█████████▉| 1000/1010 [03:57<00:02,  4.22it/s, loss=0.114, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9:  99%|█████████▉| 1002/1010 [03:58<00:01,  4.21it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:06,  1.22it/s]\u001b[A\n",
      "Epoch 9:  99%|█████████▉| 1004/1010 [03:59<00:01,  4.19it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:03<00:05,  1.09it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1006/1010 [04:01<00:00,  4.17it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:04<00:02,  1.41it/s]\u001b[A\n",
      "Epoch 9: 100%|█████████▉| 1008/1010 [04:02<00:00,  4.16it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:05<00:01,  1.70it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 1010/1010 [04:03<00:00,  4.15it/s, loss=0.114, v_num=q6d8]\n",
      "Epoch 9: 100%|██████████| 1010/1010 [04:04<00:00,  4.14it/s, loss=0.114, v_num=q6d8]\n",
      "Epoch 10:  99%|█████████▉| 1000/1010 [03:56<00:02,  4.23it/s, loss=0.113, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 10:  99%|█████████▉| 1002/1010 [03:57<00:01,  4.22it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:05,  1.42it/s]\u001b[A\n",
      "Epoch 10:  99%|█████████▉| 1004/1010 [03:58<00:01,  4.21it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.51it/s]\u001b[A\n",
      "Epoch 10: 100%|█████████▉| 1006/1010 [03:59<00:00,  4.19it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:03<00:02,  1.80it/s]\u001b[A\n",
      "Epoch 10: 100%|█████████▉| 1008/1010 [04:00<00:00,  4.19it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:04<00:00,  2.08it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 1010/1010 [04:01<00:00,  4.18it/s, loss=0.113, v_num=q6d8]\n",
      "Epoch 10: 100%|██████████| 1010/1010 [04:02<00:00,  4.17it/s, loss=0.113, v_num=q6d8]\n",
      "Epoch 11:  99%|█████████▉| 1000/1010 [03:57<00:02,  4.22it/s, loss=0.113, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 11:  99%|█████████▉| 1002/1010 [03:58<00:01,  4.21it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:05,  1.53it/s]\u001b[A\n",
      "Epoch 11:  99%|█████████▉| 1004/1010 [03:59<00:01,  4.20it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.55it/s]\u001b[A\n",
      "Epoch 11: 100%|█████████▉| 1006/1010 [04:00<00:00,  4.18it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:03<00:02,  1.89it/s]\u001b[A\n",
      "Epoch 11: 100%|█████████▉| 1008/1010 [04:01<00:00,  4.18it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:04<00:00,  2.16it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 1010/1010 [04:02<00:00,  4.17it/s, loss=0.113, v_num=q6d8]\n",
      "Epoch 11: 100%|██████████| 1010/1010 [04:02<00:00,  4.16it/s, loss=0.113, v_num=q6d8]\n",
      "Epoch 12:  99%|█████████▉| 1000/1010 [03:57<00:02,  4.21it/s, loss=0.114, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 12:  99%|█████████▉| 1002/1010 [03:58<00:01,  4.20it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:05,  1.54it/s]\u001b[A\n",
      "Epoch 12:  99%|█████████▉| 1004/1010 [03:59<00:01,  4.19it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.57it/s]\u001b[A\n",
      "Epoch 12: 100%|█████████▉| 1006/1010 [04:00<00:00,  4.17it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:03<00:02,  1.87it/s]\u001b[A\n",
      "Epoch 12: 100%|█████████▉| 1008/1010 [04:01<00:00,  4.17it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:04<00:00,  2.18it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 1010/1010 [04:02<00:00,  4.16it/s, loss=0.114, v_num=q6d8]\n",
      "Epoch 12: 100%|██████████| 1010/1010 [04:03<00:00,  4.15it/s, loss=0.114, v_num=q6d8]\n",
      "Epoch 13:  99%|█████████▉| 1000/1010 [04:09<00:02,  4.01it/s, loss=0.113, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 13:  99%|█████████▉| 1002/1010 [04:09<00:01,  4.01it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:05,  1.56it/s]\u001b[A\n",
      "Epoch 13:  99%|█████████▉| 1004/1010 [04:11<00:01,  4.00it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.58it/s]\u001b[A\n",
      "Epoch 13: 100%|█████████▉| 1006/1010 [04:12<00:01,  3.99it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:03<00:02,  1.86it/s]\u001b[A\n",
      "Epoch 13: 100%|█████████▉| 1008/1010 [04:13<00:00,  3.98it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:04<00:00,  2.20it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 1010/1010 [04:13<00:00,  3.98it/s, loss=0.113, v_num=q6d8]\n",
      "Epoch 13: 100%|██████████| 1010/1010 [04:14<00:00,  3.97it/s, loss=0.113, v_num=q6d8]\n",
      "Epoch 14:  99%|█████████▉| 1000/1010 [04:02<00:02,  4.13it/s, loss=0.114, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 14:  99%|█████████▉| 1002/1010 [04:03<00:01,  4.12it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:05,  1.54it/s]\u001b[A\n",
      "Epoch 14:  99%|█████████▉| 1004/1010 [04:04<00:01,  4.11it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.56it/s]\u001b[A\n",
      "Epoch 14: 100%|█████████▉| 1006/1010 [04:05<00:00,  4.10it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:03<00:02,  1.91it/s]\u001b[A\n",
      "Epoch 14: 100%|█████████▉| 1008/1010 [04:06<00:00,  4.09it/s, loss=0.114, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:04<00:00,  2.22it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 1010/1010 [04:07<00:00,  4.09it/s, loss=0.114, v_num=q6d8]\n",
      "Epoch 14: 100%|██████████| 1010/1010 [04:07<00:00,  4.08it/s, loss=0.114, v_num=q6d8]\n",
      "Epoch 15:  99%|█████████▉| 1000/1010 [04:02<00:02,  4.13it/s, loss=0.113, v_num=q6d8]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 15:  99%|█████████▉| 1002/1010 [04:02<00:01,  4.12it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  20%|██        | 2/10 [00:01<00:05,  1.55it/s]\u001b[A\n",
      "Epoch 15:  99%|█████████▉| 1004/1010 [04:04<00:01,  4.11it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  40%|████      | 4/10 [00:02<00:03,  1.63it/s]\u001b[A\n",
      "Epoch 15: 100%|█████████▉| 1006/1010 [04:05<00:00,  4.10it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  60%|██████    | 6/10 [00:03<00:02,  1.95it/s]\u001b[A\n",
      "Epoch 15: 100%|█████████▉| 1008/1010 [04:05<00:00,  4.10it/s, loss=0.113, v_num=q6d8]\n",
      "Validating:  80%|████████  | 8/10 [00:04<00:00,  2.25it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 1010/1010 [04:06<00:00,  4.09it/s, loss=0.113, v_num=q6d8]\n",
      "Epoch 15: 100%|██████████| 1010/1010 [04:07<00:00,  4.08it/s, loss=0.113, v_num=q6d8]\n",
      "Epoch 16:  74%|███████▍  | 751/1010 [03:00<01:02,  4.16it/s, loss=0.114, v_num=q6d8] "
     ]
    }
   ],
   "source": [
    "kaiming_init(model)\n",
    "logger = WandbLogger(project=\"ITk_dual_embedding\")\n",
    "trainer = Trainer(gpus=1, max_epochs=hparams[\"max_epochs\"], logger=logger, num_sanity_val_steps=2, callbacks=[checkpoint_callback], log_every_n_steps = 50, default_root_dir=\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_embedding/\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.finish()\n",
    "\n",
    "with open(\"dual_embedding_default.yaml\") as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "hparams[\"use_dual_encoder\"] = True\n",
    "\n",
    "model = VanillaDualEmbedding(hparams)\n",
    "\n",
    "checkpoint = torch.load(\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_embedding/ITk_dual_embedding/3ijb4qnw/checkpoints/last.ckpt\")\n",
    "state_dict = checkpoint[\"state_dict\"]\n",
    "names = [i for i in state_dict]\n",
    "for i in names:\n",
    "    state = state_dict[i]\n",
    "    i = i.replace(\"input_layer1\", \"input_layer2\")\n",
    "    i = i.replace(\"layers1\", \"layers2\")\n",
    "    i = i.replace(\"output_layer1\", \"output_layer2\")\n",
    "    state_dict[i] = state\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='pur',\n",
    "    mode=\"max\",\n",
    "    save_top_k=2,\n",
    "    save_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mexatrkx\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.11 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/exatrkx/ITk_dual_embedding/runs/1laruh1o\" target=\"_blank\">charmed-star-32</a></strong> to <a href=\"https://wandb.ai/exatrkx/ITk_dual_embedding\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | cos           | CosineSimilarity | 0     \n",
      "1 | input_layer1  | Linear           | 12.3 K\n",
      "2 | layers1       | ModuleList       | 5.2 M \n",
      "3 | output_layer1 | Linear           | 16.4 K\n",
      "4 | input_layer2  | Linear           | 12.3 K\n",
      "5 | layers2       | ModuleList       | 5.2 M \n",
      "6 | output_layer2 | Linear           | 16.4 K\n",
      "7 | act           | GELU             | 0     \n",
      "---------------------------------------------------\n",
      "10.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.6 M    Total params\n",
      "42.214    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:  50%|█████     | 1/2 [00:01<00:01,  1.46s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 5. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/r/ryanliu/.conda/envs/gnn/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  99%|█████████▉| 1000/1010 [03:52<00:02,  4.29it/s, loss=0.0123, v_num=uh1o]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1002/1010 [03:56<00:01,  4.24it/s, loss=0.0123, v_num=uh1o]\n",
      "Validating:  20%|██        | 2/10 [00:05<00:19,  2.42s/it]\u001b[A\n",
      "Epoch 0:  99%|█████████▉| 1004/1010 [04:00<00:01,  4.18it/s, loss=0.0123, v_num=uh1o]\n",
      "Validating:  40%|████      | 4/10 [00:09<00:13,  2.19s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1006/1010 [04:04<00:00,  4.11it/s, loss=0.0123, v_num=uh1o]\n",
      "Validating:  60%|██████    | 6/10 [00:13<00:08,  2.17s/it]\u001b[A\n",
      "Epoch 0: 100%|█████████▉| 1008/1010 [04:08<00:00,  4.06it/s, loss=0.0123, v_num=uh1o]\n",
      "Validating:  80%|████████  | 8/10 [00:16<00:03,  1.80s/it]\u001b[A\n",
      "Epoch 0: 100%|██████████| 1010/1010 [04:11<00:00,  4.02it/s, loss=0.0123, v_num=uh1o]\n",
      "Epoch 0: 100%|██████████| 1010/1010 [04:13<00:00,  3.98it/s, loss=0.0123, v_num=uh1o]\n",
      "Epoch 1:  99%|█████████▉| 1000/1010 [04:04<00:02,  4.09it/s, loss=0.0115, v_num=uh1o]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 1002/1010 [04:07<00:01,  4.04it/s, loss=0.0115, v_num=uh1o]\n",
      "Validating:  20%|██        | 2/10 [00:05<00:19,  2.45s/it]\u001b[A\n",
      "Epoch 1:  99%|█████████▉| 1004/1010 [04:11<00:01,  3.99it/s, loss=0.0115, v_num=uh1o]\n",
      "Validating:  40%|████      | 4/10 [00:10<00:15,  2.63s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1006/1010 [04:17<00:01,  3.91it/s, loss=0.0115, v_num=uh1o]\n",
      "Validating:  60%|██████    | 6/10 [00:14<00:09,  2.29s/it]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 1008/1010 [04:20<00:00,  3.86it/s, loss=0.0115, v_num=uh1o]\n",
      "Validating:  80%|████████  | 8/10 [00:18<00:03,  1.96s/it]\u001b[A\n",
      "Epoch 1: 100%|██████████| 1010/1010 [04:24<00:00,  3.82it/s, loss=0.0115, v_num=uh1o]\n",
      "Epoch 1: 100%|██████████| 1010/1010 [04:26<00:00,  3.79it/s, loss=0.0115, v_num=uh1o]\n",
      "Epoch 2:  99%|█████████▉| 1000/1010 [04:16<00:02,  3.89it/s, loss=0.0114, v_num=uh1o]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 1002/1010 [04:19<00:02,  3.85it/s, loss=0.0114, v_num=uh1o]\n",
      "Validating:  20%|██        | 2/10 [00:05<00:19,  2.41s/it]\u001b[A\n",
      "Epoch 2:  99%|█████████▉| 1004/1010 [04:24<00:01,  3.80it/s, loss=0.0114, v_num=uh1o]\n",
      "Validating:  40%|████      | 4/10 [00:09<00:14,  2.34s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1006/1010 [04:29<00:01,  3.74it/s, loss=0.0114, v_num=uh1o]\n",
      "Validating:  60%|██████    | 6/10 [00:13<00:08,  2.20s/it]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 1008/1010 [04:32<00:00,  3.70it/s, loss=0.0114, v_num=uh1o]\n",
      "Validating:  80%|████████  | 8/10 [00:17<00:03,  1.97s/it]\u001b[A\n",
      "Epoch 2: 100%|██████████| 1010/1010 [04:36<00:00,  3.65it/s, loss=0.0114, v_num=uh1o]\n",
      "Epoch 2: 100%|██████████| 1010/1010 [04:38<00:00,  3.62it/s, loss=0.0114, v_num=uh1o]\n",
      "Epoch 3:  10%|▉         | 97/1010 [00:26<04:04,  3.73it/s, loss=0.0114, v_num=uh1o]  "
     ]
    }
   ],
   "source": [
    "logger = WandbLogger(project=\"ITk_dual_embedding\")\n",
    "trainer = Trainer(gpus=1, max_epochs=hparams[\"max_epochs\"], logger=logger, num_sanity_val_steps=2, callbacks=[checkpoint_callback], log_every_n_steps = 50, default_root_dir=\"/global/cfs/cdirs/m3443/usr/ryanliu/ITk_embedding/\")\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
